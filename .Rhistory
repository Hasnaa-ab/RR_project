# Preprocessing for bigrams and trigrams
dtm_plot <- dtm %>%
as.matrix() %>%
as.data.frame() %>%
rownames_to_column(var = "document") %>%
mutate(document = as.character(document))
# -------------
# Preprocessing for bigrams and trigrams
dtm <- dtm %>%
as.matrix() %>%
as.data.frame()
# Add a "document" column with row names
topics <- data.frame(document = rownames(dtm), dtm)
# Convert the document-term matrix to a tidy format
topics <- tidy(topics, document) %>%
mutate(word = as.character(word)) %>%
filter(!is.na(word) & word != "") %>%
filter(!word %in% stop_words$word)
################## function to read txt and return data frame
multiTextFile <- function(directoryPath) {
# Get the list of file names in the directory
fileNames <- list.files(path = directoryPath, pattern = "\\.txt$", full.names = TRUE)
# Create an empty list to store the data frames
dataFrames <- list()
# Read each file and store it as a separate data frame
for (filePath in fileNames) {
data <- read.table(filePath, header = TRUE)  # Modify read function based on your file format
# Add the data frame to the list
dataFrames <- c(dataFrames, list(data))
}
# Merge the data frames into a single data frame
mergedData <- do.call(rbind, dataFrames)
names(mergedData) <- 'text'
mergedData <- mergedData[!duplicated(mergedData$text), ]
# Return the merged data frame
return(mergedData)
}
############################# function to read csv with text
csvText <- function(file, textCol, labelCol) {
data <- read.csv(file)
data = data[,c(textCol,labelCol)]
names(data) = c('text','label')
names(data[,labelCol]) = 'label'
data <- data[!duplicated(data$text), ]
data$text = as.character(data$text)
data$label = as.factor(data$label)
return(data)
}
myData = csvText('fulldata-updated.csv', 'title', 'label')
######################### merge label col to data frame
assignLabels <- function(df,labels) {
df <- cbind(df,labels)
names(df)[2] = 'label'
return(df)
}
######################drop unique labels
validLabels <- function(df) {
labelcount <- table(df$label)
repeated <- names(labelcount[labelcount > 1])
df <- df[df$label %in% repeated, ]
df <- droplevels(df)  # Drop unused levels if needed
rownames(df) <- seq_len(nrow(df))
return(df)
}
myData <- validLabels(myData)
############################ cleaning Text
cleanText <- function(data) {
data$text <- str_replace_all(data$text, "\n", " ")
data$text <- str_replace_all(data$text, "[0-9]+", "")
data$text <- str_replace_all(data$text, "[,\\!?/:;''()``’“-”—#]", "")
data$text <- str_replace_all(data$text, "[.]+", "")
data$text <- tolower(data$text)
data$text <- str_replace_all(data$text, "\\b\\w\\b", "")
data$text <- as.character(data$text)
return(data$text)
}
articles <- cleanText(myData)
################################# create tokens
# Tokenizer
articles <- sapply(articles, function(x) tokenizers::tokenize_words(x))
articles <- as.list(articles)
#-------------
# Combine the single tokens into a single string per document
articlesflat <- sapply(articles, paste, collapse = " ")
articlesflat
library(textrecipes)
install.packages('textrecipes')
library(textrecipes)
# Create a recipe
recipe <- recipe(text ~ ., data = topics) %>%
step_tokenize(text) %>%
step_stopwords(text, custom_stopwords = stopwordscustom) %>%
step_ngram(text, n = 2:3) %>%
step_tfidf(text)
# Create a corpus from the articlesflat
corpus <- Corpus(VectorSource(articlesflat))
# Preprocess the corpus
corpus <- corpus %>%
tm_map(content_transformer(tolower)) %>%
tm_map(removeNumbers) %>%
tm_map(removePunctuation) %>%
tm_map(removeWords, stopwords("en")) %>%
tm_map(stripWhitespace)
# Create the document-term matrix (DTM) with n-grams
dtm <- DocumentTermMatrix(corpus, control = list(
tokenizer = BigramTokenizer(),
ngrams = c(2, 3)
))
#-------------
# Combine the single tokens into a single string per document
library(tidytext)
articlesflat <- sapply(articles, paste, collapse = " ")
# Create a corpus from the articlesflat
corpus <- Corpus(VectorSource(articlesflat))
# Preprocess the corpus
corpus <- corpus %>%
tm_map(content_transformer(tolower)) %>%
tm_map(removeNumbers) %>%
tm_map(removePunctuation) %>%
tm_map(removeWords, stopwords("en")) %>%
tm_map(stripWhitespace)
# Create the document-term matrix (DTM) with n-grams
dtm <- DocumentTermMatrix(corpus, control = list(
tokenizer = BigramTokenizer(),
ngrams = c(2, 3)
))
# Define a custom tokenizer function to generate bigrams and trigrams
customTokenizer <- function(x) {
unlist(tidytext::tokenize_ngrams(x, n = 2:3))
}
# Create a corpus from the articlesflat
corpus <- Corpus(VectorSource(articlesflat))
# Preprocess the corpus
corpus <- corpus %>%
tm_map(content_transformer(tolower)) %>%
tm_map(removeNumbers) %>%
tm_map(removePunctuation) %>%
tm_map(removeWords, stopwords("en")) %>%
tm_map(stripWhitespace)
# Create the document-term matrix (DTM) with n-grams
dtm <- DocumentTermMatrix(corpus, control = list(
tokenizer = customTokenizer
))
# Create a corpus from the articlesflat
corpus <- corpus(articlesflat)
library(quanteda)
# Create a corpus from the articlesflat
corpus <- corpus(articlesflat)
# Preprocess the corpus
corpus <- corpus %>%
tokens(remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE) %>%
tokens_tolower() %>%
tokens_remove(stopwords("en")) %>%
tokens_ngrams(n = 2:3, concatenator = "_")
articlesflat
articles <- cleanText(myData)
articlesflat <- sapply(articles, paste, collapse = " ")
articlesflat
articles
# Tokenizer
articles <- sapply(articles, function(x) tokenizers::tokenize_words(x))
articles <- as.list(articles)
articlesflat <- sapply(articles, paste, collapse = " ")
articlesflat
# Create a corpus from the articlesflat
corpus <- corpus(articlesflat)
articles <- cleanText(myData)
#-------------
# Tokenize the text to unigrams
unigrams <- sapply(articles, function(x) tokenizers::tokenize_words(x))
unigrams
# Create bigrams and trigrams
bigrams <- lapply(unigrams, function(tokens) {
tokens_bigrams <- tokenizers::tokenize_ngrams(tokens, n = 2)
str_replace_all(tokens_bigrams, " ", "_")
})
bigrams
#-------------
# Tokenize the text to unigrams
unigrams <- sapply(articles, function(x) tokenizers::tokenize_words(x))
# Create bigrams and trigrams
bigrams <- lapply(unigrams, function(tokens) {
tokens_bigrams <- tokens %>%
tidytext::pairwise_bigrams() %>%
unite("bigram", c("word1", "word2"), sep = " ")
str_replace_all(tokens_bigrams, " ", "_")
})
#-------------
# Tokenize the text to unigrams
unigrams <- sapply(articles, function(x) tokenizers::tokenize_words(x))
# Create bigrams and trigrams
bigrams <- lapply(unigrams, function(tokens) {
tokens_bigrams <- tokens %>%
tidytext::pairwise_bigrams() %>%
mutate(bigram = paste(word1, word2, sep = "_"))
str_replace_all(tokens_bigrams$bigram, " ", "_")
})
#-------------
# Tokenize the text to unigrams
unigrams <- sapply(articles, function(x) tokenizers::tokenize_words(x))
# Create bigrams and trigrams
bigrams <- lapply(unigrams, function(tokens) {
tokens_bigrams <- tokenizers::tokenize_ngrams(tokens, n = 2, concatenator = "_")
str_replace_all(tokens_bigrams, " ", "_")
})
#-------------
# Tokenize the text to unigrams
unigrams <- sapply(articles, function(x) tokenizers::tokenize_words(x))
# Create bigrams and trigrams
bigrams <- lapply(unigrams, function(tokens) {
tokens_bigrams <- tokenizers::tokenize_ngrams(tokens, n = 2, ngram_delim = "_")
str_replace_all(tokens_bigrams, " ", "_")
})
trigrams <- lapply(unigrams, function(tokens) {
tokens_trigrams <- tokenizers::tokenize_ngrams(tokens, n = 3, ngram_delim = "_")
str_replace_all(tokens_trigrams, " ", "_")
})
# Combine unigrams, bigrams, and trigrams
ngrams <- Map(c, unigrams, bigrams, trigrams)
ngrams
#-------------
# Tokenize the text to unigrams
unigrams <- sapply(articles, function(x) tokenizers::tokenize_words(x))
# Create bigrams and trigrams
bigrams <- lapply(unigrams, function(tokens) {
tokens_bigrams <- tokenizers::tokenize_ngrams(tokens, n = 2, ngram_delim = "_")
tokens_bigrams <- sapply(tokens_bigrams, paste, collapse = " ")
str_replace_all(tokens_bigrams, "\\s+", "_")
})
bigrams
bigrams <- sapply(articles, function(x) tokenizers::tokenize_words(x, n=2, ngram_delim = '-'))
# Create bigrams
bigrams <- lapply(unigrams, function(tokens) {
tokens_bigrams <- tokenizers::tokenize_skip_ngrams(tokens, n = 2, n_min = 2, separator = "_")
str_replace_all(tokens_bigrams, "\\s+", "_")
})
tokens_bigrams <- tokenizers::tokenize_skip_ngrams(tokens, n = 2, n_min = 2, ngram_delim = "_")
# Create bigrams
bigrams <- lapply(unigrams, function(tokens) {
tokens_bigrams <- tokenizers::tokenize_skip_ngrams(tokens, n = 2, n_min = 2, ngram_delim = "_")
str_replace_all(tokens_bigrams, "\\s+", "_")
})
# Create bigrams
bigrams <- lapply(unigrams, function(tokens) {
tokens_bigrams <- tokenizers::tokenize_skip_ngrams(tokens, n = 2, n_min = 2)
str_replace_all(tokens_bigrams, "\\s+", "_")
})
bigrams
# Create bigrams
bigrams <- lapply(articles, function(tokens) {
tokens_bigrams <- tokenizers::tokenize_skip_ngrams(tokens, n = 2, n_min = 2)
str_replace_all(tokens_bigrams, "\\s+", "_")
})
bigrams
# Create bigrams
bigrams <- lapply(articles, function(tokens) {
tokens_bigrams <- tokenizers::tokenize_skip_ngrams(tokens, n = 2)
})
bigrams
# Create bigrams
bigrams <- lapply(articles, function(articles) {
tokens_bigrams <- tokenizers::tokenize_skip_ngrams(articles, n = 2)
})
bigrams
trigrams <- lapply(articles, function(articles) {
tokens_bigrams <- tokenizers::tokenize_skip_ngrams(articles, n = 3)
})
trigrams
articles <- cleanText(myData)
# Tokenizer
articles <- sapply(articles, function(x) tokenizers::tokenize_words(x, n=3))
# Tokenizer
articles <- sapply(articles, function(x,n) tokenizers::tokenize_words(x,n=3))
trigrams <- lapply(articles, function(articles) {
tokens_bigrams <- tokenizers::tokenize_skip_ngrams(articles, n = 3)
})
trigrams
# Tokenizer
articles <- lapply(articles, function(x) tokenizers::tokenize_words(x,n=3))
articles <- cleanText(myData)
stemmed_articles <- list()
for (i in 1:length(articles)) {
words <- c()
for (word in articles[[i]]) {
stemmed_word <- SnowballC::wordStem(word, "english")
words <- c(words, stemmed_word)
}
stemmed_articles[[i]] <- words
}
stemmed_articles <- as.list(stemmed_articles)
# Create custom stopwords
stopwordscustom <- read.csv('stp.csv', header = FALSE, col.names = c('word'))
stopwordscustom <- as.character(stopwordscustom$word)
# Remove custom stopwords
articles <- lapply(articles, function(x) x[!x %in% stopwordscustom])
articles
trigrams <- lapply(articles, function(articles) {
tokens_bigrams <- tokenizers::tokenize_skip_ngrams(articles, n = 3)
})
trigrams
# Remove custom stopwords
articles <- lapply(stemmed_articles, function(x) x[!x %in% stopwordscustom])
trigrams <- lapply(articles, function(articles) {
tokens_bigrams <- tokenizers::tokenize_skip_ngrams(articles, n = 3)
})
trigrams
dtm <- DocumentTermMatrix(Corpus(VectorSource(trigrams)))
dtm
# Transform to a term frequency-inverse document frequency (TF-IDF) matrix
dtm_tfidf <- weightTfIdf(dtm)
# Label encoding
myData$labelnumber <- as.numeric(as.factor(myData$label))
# Merge the labels with the dtm
dtm_tfidf <- cbind(dtm_tfidf, myData$labelnumber)
# Split the dataset into training and testing sets
set.seed(4545) #seed for the reproducibility
trainIndex <- createDataPartition(myData$labelnumber, p = .6, list = FALSE, times = 1)
# Split the labels as well
train_labels <- myData$labelnumber[trainIndex]
test_labels  <- myData$labelnumber[-trainIndex]
# Split the dtm_tfidf
train <- dtm_tfidf[trainIndex,]
test <- dtm_tfidf[-trainIndex,]
# Prepare matrices suitable for Random Forest
train_df <- as.data.frame(as.matrix(train))
test_df <- as.data.frame(as.matrix(test))
colnames(train_df)[ncol(train_df)] <- "labelnumer"
colnames(test_df)[ncol(test_df)] <- "labelnumer"
# Rename the columns to use only alphanumeric characters and underscores
names(train_df) <- make.names(names(train_df), unique = TRUE)
names(test_df) <- make.names(names(test_df), unique = TRUE)
# Train the model
model <- ranger(as.factor(train_labels) ~ ., data = train_df,
importance = 'impurity', num.trees = 500)
# Predict on the test set
predictions <- predict(model, test_df)
# Evaluate model performance
table(predictions$predictions, test_labels)
# Check accuracy
accuracy <- sum(predictions$predictions == test_labels) / length(test_labels)
accuracy
# FIX ATTEMPT
# Define the levels that should exist
all_levels <- 1:11 # Adjust this to the levels we expect to have
# Convert predictions and test_labels to factor and explicitly set the levels
predictions_factor <- factor(predictions$predictions, levels=all_levels)
test_labels_factor <- factor(test_labels, levels=all_levels)
# Compute the confusion matrix
cm <- confusionMatrix(predictions_factor, test_labels_factor)
# Print the confusion matrix
print(cm)
# Run a Bagging model
control <- trainControl(method = "cv", number = 2) # Changed method to 'cv' for cross-validation and number to 2 for 2-fold cross-validation, as it is computationally heavy.
model_bag <- caret::train(as.factor(labelnumer) ~ ., data=train_df, trControl=control, method="treebag")
predictions_bag <- predict(model_bag, newdata = test_df, type="raw")
# Run a LDA model and plot the topics
lda <- LDA(dtm, k = 20, control = list(seed = 3434), )
topics <- tidytext::tidy(lda, matrix = "beta")
top_terms <- topics %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
plot_lda <- top_terms %>%
mutate(term = tidytext::reorder_within(term, beta, topic)) %>%
ggplot(aes(beta, term, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
# coord_flip() +
theme_minimal() +
labs(title = "Top 10 terms in each LDA topic",
x = "Beta", y = "")
print(plot_lda)
top_terms
# Clean the terms in the top_terms tibble by removing quotes, commas, c(, and )
top_terms <- top_terms %>%
mutate(term = gsub("\"|,|\\bc\\(|\\)", "", term)) # Remove quotes, commas, c(, and )
plot_lda <- top_terms %>%
mutate(term = tidytext::reorder_within(term, beta, topic)) %>%
ggplot(aes(beta, term, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
# coord_flip() +
theme_minimal() +
labs(title = "Top 10 terms in each LDA topic",
x = "Beta", y = "")
print(plot_lda)
View(dtm)
dtm.dimnames
dtm[dimnames]
trigrams <- unlist(trigrams)
trigrams
dtm <- DocumentTermMatrix(Corpus(VectorSource(trigrams)))
# Transform to a term frequency-inverse document frequency (TF-IDF) matrix
dtm_tfidf <- weightTfIdf(dtm)
# Label encoding
myData$labelnumber <- as.numeric(as.factor(myData$label))
trigrams <- lapply(articles, function(articles) {
tokens_trigrams <- tokenizers::tokenize_skip_ngrams(articles, n = 3)
})
trigrams
# Tokenize into trigrams
trigrams <- sapply(articles, function(x) tokenizers::tokenize_ngrams(x, n = 3))
trigrams
trigrams
trigrams <- unlist(trigrams)
trigrams
#trigrams <- unlist(trigrams)
# Convert trigrams into a Corpus
corpus <- Corpus(VectorSource(trigrams))
dtm <- DocumentTermMatrix(corpus)
# Transform to a term frequency-inverse document frequency (TF-IDF) matrix
dtm_tfidf <- weightTfIdf(dtm)
# Label encoding
myData$labelnumber <- as.numeric(as.factor(myData$label))
# Merge the labels with the dtm
dtm_tfidf <- cbind(dtm_tfidf, myData$labelnumber)
dtm <- DocumentTermMatrix(corpus)
# Find which documents are empty
empty_docs <- which(row_sums(as.matrix(dtm)) == 0)
# Remove empty documents
dtm <- dtm[-empty_docs, ]
# Now compute the TF-IDF
dtm_tfidf <- weightTfIdf(dtm)
# Label encoding
myData$labelnumber <- as.numeric(as.factor(myData$label))
# Merge the labels with the dtm
dtm_tfidf <- cbind(dtm_tfidf, myData$labelnumber)
# Remove corresponding labels
filtered_labelnumber <- myData$labelnumber[-empty_docs]
# Now combine the labels with the dtm_tfidf
dtm_tfidf <- cbind(dtm_tfidf, filtered_labelnumber)
dtm
# Remove empty documents
dtm <- dtm[-empty_docs, ]
# Now compute the TF-IDF
dtm_tfidf <- weightTfIdf(dtm)
# Remove corresponding labels
filtered_labelnumber <- myData$labelnumber[-empty_docs]
# Now combine the labels with the dtm_tfidf
dtm_tfidf <- cbind(dtm_tfidf, filtered_labelnumber)
#trigrams <- unlist(trigrams)
# Convert trigrams into a Corpus
corpus <- Corpus(VectorSource(trigrams))
dtm <- DocumentTermMatrix(corpus)
# Find which documents are empty
empty_docs <- which(row_sums(as.matrix(dtm)) == 0)
# Remove empty documents
dtm <- dtm[-empty_docs, ]
# Now compute the TF-IDF
dtm_tfidf <- weightTfIdf(dtm)
# Remove corresponding labels
filtered_labelnumber <- myData$labelnumber[-empty_docs]
# Now combine the labels with the dtm_tfidf
dtm_tfidf <- cbind(dtm_tfidf, filtered_labelnumber)
# Check the dimensions
cat("Number of rows in dtm_tfidf:", nrow(dtm_tfidf), "\n")
cat("Length of filtered_labelnumber:", length(filtered_labelnumber), "\n")
# Tokenize into trigrams
trigrams <- sapply(articles, function(x) tokenizers::tokenize_ngrams(x, n = 3))
trigrams
#trigrams <- unlist(trigrams)
#trigrams <- unlist(trigrams)
# Convert trigrams into a Corpus
corpus <- Corpus(VectorSource(trigrams))
dtm <- DocumentTermMatrix(corpus)
# Now compute the TF-IDF
dtm_tfidf <- weightTfIdf(dtm)
# Label encoding
myData$labelnumber <- as.numeric(as.factor(myData$label))
# Merge the labels with the dtm
dtm_tfidf <- cbind(dtm_tfidf, myData$labelnumber)
# Split the dataset into training and testing sets
set.seed(4545) #seed for the reproducibility
trainIndex <- createDataPartition(myData$labelnumber, p = .6, list = FALSE, times = 1)
# Split the labels as well
train_labels <- myData$labelnumber[trainIndex]
test_labels  <- myData$labelnumber[-trainIndex]
# Split the dtm_tfidf
train <- dtm_tfidf[trainIndex,]
test <- dtm_tfidf[-trainIndex,]
# Prepare matrices suitable for Random Forest
train_df <- as.data.frame(as.matrix(train))
test_df <- as.data.frame(as.matrix(test))
colnames(train_df)[ncol(train_df)] <- "labelnumer"
colnames(test_df)[ncol(test_df)] <- "labelnumer"
# Rename the columns to use only alphanumeric characters and underscores
names(train_df) <- make.names(names(train_df), unique = TRUE)
names(test_df) <- make.names(names(test_df), unique = TRUE)
# Train the model
model <- ranger(as.factor(train_labels) ~ ., data = train_df,
importance = 'impurity', num.trees = 500)
# Predict on the test set
predictions <- predict(model, test_df)
# Evaluate model performance
table(predictions$predictions, test_labels)
# Check accuracy
accuracy <- sum(predictions$predictions == test_labels) / length(test_labels)
accuracy
# FIX ATTEMPT
# Define the levels that should exist
all_levels <- 1:11 # Adjust this to the levels we expect to have
# Convert predictions and test_labels to factor and explicitly set the levels
predictions_factor <- factor(predictions$predictions, levels=all_levels)
test_labels_factor <- factor(test_labels, levels=all_levels)
# Compute the confusion matrix
cm <- confusionMatrix(predictions_factor, test_labels_factor)
# Print the confusion matrix
print(cm)
# Run a Bagging model
control <- trainControl(method = "cv", number = 2) # Changed method to 'cv' for cross-validation and number to 2 for 2-fold cross-validation, as it is computationally heavy.
model_bag <- caret::train(as.factor(labelnumer) ~ ., data=train_df, trControl=control, method="treebag")
