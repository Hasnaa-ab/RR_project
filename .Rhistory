<<<<<<< HEAD
library(stringr)
library(tokenizers)
library(SnowballC)
fulldata <- read.csv("fulldata-updated.csv")
fulldata <- fulldata[!duplicated(fulldata$title), ]
fulldata <- fulldata[order(rownames(fulldata)), ]
fulldata <- as.data.frame(lapply(fulldata, type.convert))
rownames(fulldata) <- NULL
source("C:/Users/Admin/Desktop/RR/RRProject/RR_project/textcorpus_reproduced.R", echo=TRUE)
# Clean text
articles <- str_replace_all(fulldata$article, "\n", " ")
articles <- str_replace_all(articles, "[0-9]+", "")
articles <- str_replace_all(articles, "[,\\!?/:;''()``’“-”—#]", "")
articles <- str_replace_all(articles, "[.]+", "")
articles <- tolower(articles)
articles <- str_replace_all(articles, "\\b\\w\\b", "")
articles <- as.character(articles)
# Tokenizer
articles <- sapply(articles, function(x) tokenizers::tokenize_words(x))
articles <- as.list(articles)
stemmed_articles <- list()
for (i in 1:length(articles)) {
words <- c()
for (word in articles[[i]]) {
stemmed_word <- SnowballC::wordStem(word, "english")
words <- c(words, stemmed_word)
}
stemmed_articles[[i]] <- words
}
stemmed_articles <- as.list(stemmed_articles)
#
stopwordscustom <- read.csv('stp.csv', header = FALSE, col.names = c('word'))
stopwordscustom <- as.character(stopwordscustom$word)
# Install and load necessary packages
#install.packages(c("tm", "SnowballC", "slam", "topicmodels", "quanteda", "caret", "e1071", "randomForest", "kernlab", "cluster", "topicmodels", "LDAvis", "ggplot2"))
library(tm)
library(SnowballC)
# Install and load necessary packages
install.packages(c("tm", "SnowballC", "slam", "topicmodels", "quanteda", "caret", "e1071", "randomForest", "kernlab", "cluster", "topicmodels", "LDAvis", "ggplot2"))
install.packages(c("tm", "SnowballC", "slam", "topicmodels", "quanteda", "caret", "e1071", "randomForest", "kernlab", "cluster", "topicmodels", "LDAvis", "ggplot2"))
library(tm)
library(SnowballC)
library(slam)
library(topicmodels)
library(quanteda)
library(caret)
library(ggplot2)
# Remove custom stopwords
articles <- lapply(articles, function(x) x[!x %in% stopwordscustom])
# Generate a document-term matrix
dtm <- DocumentTermMatrix(Corpus(VectorSource(articles)))
# Transform to a term frequency-inverse document frequency (TF-IDF) matrix
dtm_tfidf <- weightTfIdf(dtm)
# Label encoding
fulldata$labelnumber <- as.numeric(as.factor(fulldata$label))
# Merge the labels with the dtm
dtm_tfidf <- cbind(dtm_tfidf, fulldata$labelnumber)
# Split the dataset into training and testing sets
set.seed(4545)
trainIndex <- createDataPartition(fulldata$labelnumber, p = .6, list = FALSE, times = 1)
library(caret)
# Install and load necessary packages
install.packages(c("tm", "SnowballC", "slam", "topicmodels", "quanteda", "caret", "e1071", "randomForest", "kernlab", "cluster", "topicmodels", "LDAvis", "ggplot2", 'rlang'))
install.packages(c("tm", "SnowballC", "slam", "topicmodels", "quanteda", "caret", "e1071", "randomForest", "kernlab", "cluster", "topicmodels", "LDAvis", "ggplot2", "rlang"))
install.packages(c("tm", "SnowballC", "slam", "topicmodels", "quanteda", "caret", "e1071", "randomForest", "kernlab", "cluster", "topicmodels", "LDAvis", "ggplot2", "rlang"))
# Each comment is a .ipynb cell
library(stringr)
library(tokenizers)
library(SnowballC)
# Load data
fulldata <- read.csv("fulldata-updated.csv")
fulldata <- fulldata[!duplicated(fulldata$title), ]
fulldata <- fulldata[order(rownames(fulldata)), ]
fulldata <- as.data.frame(lapply(fulldata, type.convert))
rownames(fulldata) <- NULL
#
labelcount <- table(fulldata$label)
repeated <- names(labelcount[labelcount > 1])
fulldata <- fulldata[fulldata$label %in% repeated, ]
fulldata <- droplevels(fulldata)  # Drop unused levels if needed
rownames(fulldata) <- seq_len(nrow(fulldata))
#
fulldata$date <- as.POSIXct(fulldata$date, format = "%Y-%m-%d %H:%M:%S")
fulldata$label <- as.factor(fulldata$label)
month_stats <- summary(as.numeric(format(fulldata$date, "%m")))
fulldata$article <- as.character(fulldata$article)
# Clean text
articles <- str_replace_all(fulldata$article, "\n", " ")
articles <- str_replace_all(articles, "[0-9]+", "")
articles <- str_replace_all(articles, "[,\\!?/:;''()``’“-”—#]", "")
articles <- str_replace_all(articles, "[.]+", "")
articles <- tolower(articles)
articles <- str_replace_all(articles, "\\b\\w\\b", "")
articles <- as.character(articles)
# Tokenizer
articles <- sapply(articles, function(x) tokenizers::tokenize_words(x))
articles <- as.list(articles)
# PorterStemmer
stemmed_articles <- list()
for (i in 1:length(articles)) {
words <- c()
for (word in articles[[i]]) {
stemmed_word <- SnowballC::wordStem(word, "english")
words <- c(words, stemmed_word)
}
stemmed_articles[[i]] <- words
}
stemmed_articles <- as.list(stemmed_articles)
#
stopwordscustom <- read.csv('stp.csv', header = FALSE, col.names = c('word'))
stopwordscustom <- as.character(stopwordscustom$word)
# Install and load necessary packages
#install.packages(c("tm", "SnowballC", "slam", "topicmodels", "quanteda", "caret", "e1071", "randomForest", "kernlab", "cluster", "topicmodels", "LDAvis", "ggplot2", 'rlang'))
library(tm)
library(SnowballC)
library(slam)
library(topicmodels)
library(quanteda)
library(caret)
library(e1071)
library(randomForest)
library(kernlab)
library(cluster)
library(topicmodels)
library(LDAvis)
library(ggplot2)
# Remove custom stopwords
articles <- lapply(articles, function(x) x[!x %in% stopwordscustom])
# Generate a document-term matrix
dtm <- DocumentTermMatrix(Corpus(VectorSource(articles)))
# Transform to a term frequency-inverse document frequency (TF-IDF) matrix
dtm_tfidf <- weightTfIdf(dtm)
# Label encoding
fulldata$labelnumber <- as.numeric(as.factor(fulldata$label))
# Merge the labels with the dtm
dtm_tfidf <- cbind(dtm_tfidf, fulldata$labelnumber)
# Split the dataset into training and testing sets
set.seed(4545)
trainIndex <- createDataPartition(fulldata$labelnumber, p = .6, list = FALSE, times = 1)
train <- dtm_tfidf[ trainIndex,]
test  <- dtm_tfidf[-trainIndex,]
# Run a Extra Trees model
model <- randomForest(as.factor(labelnumber) ~ ., data = train, ntree = 100, mtry = 2, importance = TRUE)
train_df <- as.data.frame(as.matrix(train))
# Run a Extra Trees model
model <- randomForest(as.factor(labelnumber) ~ ., data = train, ntree = 100, mtry = 2, importance = TRUE)
str(train)
# Convert the sparse matrix to a dense matrix
train_dense <- as.matrix(train)
# Convert the dense matrix to a data frame
train_df <- as.data.frame(train_dense)
# Run a Extra Trees model
model <- randomForest(as.factor(labelnumber) ~ ., data = train, ntree = 100, mtry = 2, importance = TRUE)
# Run a Extra Trees model
model <- randomForest(as.factor(labelnumber) ~ ., data = train_df, ntree = 100, mtry = 2, importance = TRUE)
# Run a Extra Trees model
model <- randomForest(as.factor(fulldata$labelnumber) ~ ., data = train_df, ntree = 100, mtry = 2, importance = TRUE)
# Add 'fulldata$labelnumber' to 'train_df'
train_df$labelnumber <- fulldata$labelnumber
model <- randomForest(as.factor(labelnumber) ~ ., data = train_df, ntree = 100, mtry = 2, importance = TRUE)
# Add 'fulldata$labelnumber' to 'train_df'
train_df$labelnumber <- fulldata$labelnumber
# Assuming that 'fulldata' is a dataframe that includes the feature vectors and labels
set.seed(123) # for reproducibility
training_indices <- caret::createDataPartition(fulldata$labelnumber, p = 0.6, list = FALSE)
train_df <- fulldata[training_indices, ]
# Add 'fulldata$labelnumber' to 'train_df'
train_df$labelnumber <- fulldata$labelnumber
View(train)
articles
dtm
dtm_tfidf
fulldata$labelnumber <- as.numeric(as.factor(fulldata$label))
fulldata$labelnumber
dtm_tfidf <- cbind(dtm_tfidf, fulldata$labelnumber)
dtm_tfidf
trainIndex
train <- dtm_tfidf[ trainIndex,]
test  <- dtm_tfidf[-trainIndex,]
# Run a Extra Trees model
model <- randomForest(as.factor(fulldata$labelnumber) ~ ., data = train, ntree = 100, mtry = 2, importance = TRUE)
# Convert the sparse matrix to a dense matrix
train_dense <- as.matrix(train)
# Convert the dense matrix to a data frame
train_df <- as.data.frame(train_dense)
train_df
train
# Run a Extra Trees model
model <- randomForest(as.factor(fulldata$labelnumber) ~ ., data = train, ntree = 100, mtry = 2, importance = TRUE)
# Run a Extra Trees model
model <- randomForest(as.factor(fulldata$labelnumber) ~ ., data = train_df, ntree = 100, mtry = 2, importance = TRUE)
# Run a Extra Trees model
model <- randomForest(as.factor(train_df$labelnumber) ~ ., data = train_df, ntree = 100, mtry = 2, importance = TRUE)
# Split the dataset into training and testing sets
set.seed(4545)
trainIndex <- createDataPartition(fulldata$labelnumber, p = .6, list = FALSE, times = 1)
# Split the labels as well
train_labels <- fulldata$labelnumber[trainIndex]
test_labels  <- fulldata$labelnumber[-trainIndex]
# Split the dtm_tfidf
train <- dtm_tfidf[trainIndex,]
test  <- dtm_tfidf[-trainIndex,]
# Convert the sparse matrix to a dense matrix
train_dense <- as.matrix(train)
# Convert the dense matrix to a data frame
train_df <- as.data.frame(train_dense)
# Now add the labels
train_df$labelnumber <- train_labels
# Run a Extra Trees model
model <- randomForest(as.factor(train_df$labelnumber) ~ ., data = train_df, ntree = 100, mtry = 2, importance = TRUE)
# Clean up column names
names(train_df) <- gsub('[^A-Za-z0-9_]', '', names(train_df))
# Run a Extra Trees model
model <- randomForest(as.factor(train_df$labelnumber) ~ ., data = train_df, ntree = 100, mtry = 2, importance = TRUE)
# Make column names unique
names(train_df) <- make.names(names(train_df), unique = TRUE)
# Run a Extra Trees model
model <- randomForest(as.factor(train_df$labelnumber) ~ ., data = train_df, ntree = 100, mtry = 2, importance = TRUE)
# Predict on the test set
predictions <- predict(model, newdata = test)
# Convert the sparse matrix to a dense matrix
test_dense <- as.matrix(test)
# Convert the dense matrix to a data frame
test_df <- as.data.frame(test_dense)
# Predict on the test set
predictions <- predict(model, newdata = test_df)
# Make column names unique
names(test_df) <- make.names(names(test_df), unique = TRUE)
# Predict on the test set
predictions <- predict(model, newdata = test_df)
# Print classification report
print(confusionMatrix(predictions, test$labelnumber))
# Predict on the test set
predictions <- predict(model, newdata = test_df)
# Ensure that the test data has the same columns as the train data
test_df <- test_df[, names(train_df), drop = FALSE]
# Predict on the test set
predictions <- predict(model, newdata = test_df)
test_df
# Ensure that the test data has the same columns as the train data
missing_cols <- setdiff(names(train_df), names(test_df))
for (col in missing_cols) {
test_df[[col]] <- 0
}
# Now add the labels
test_df$labelnumber <- test_labels
# Make column names unique
names(test_df) <- make.names(names(test_df), unique = TRUE)
# Predict on the test set
predictions <- predict(model, newdata = test_df)
train_df
install.packages('git')
library(git)
=======
# Plot the LDA topics
plot_lda <- top_terms %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(beta, term, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
theme_minimal() +
labs(title = title, x = "Beta", y = "")
print(plot_lda)
}
plot_lda_topics(articles = articles_clean, k = 10, num_top_terms = 10,
title = "Top 10 Terms in Each LDA Topic", min_ngram = 1, max_ngram = 1)
plot_lda_topics(articles = articles_clean, k = 10, num_top_terms = 10,
title = "Top 10 Terms in Each LDA Topic", min_ngram = 1, max_ngram = 2)
plot_lda_topics(articles = articles_clean, k = 10, num_top_terms = 10,
title = "Top 10 Terms in Each LDA Topic", min_ngram = 1, max_ngram = 3)
plot_lda_topics(articles = articles_clean, k = 10, num_top_terms = 10,
title = "Top 10 Terms in Each LDA Topic", min_ngram = 2, max_ngram = 2)
plot_lda_topics(articles = articles_clean, k = 10, num_top_terms = 10,
title = "Top 10 Terms in Each LDA Topic", min_ngram = 1, max_ngram = 2)
stemmed_articles <- stem_articles(articles_clean)
# Create a Corpus
corpus <- VCorpus(VectorSource(stemmed_articles))
# Define a custom tokenizer function for bigrams and trigrams
myTokenizer <- function(x) {
NGramTokenizer(x, Weka_control(min = 1, max = 2))
}
# Create a Document-Term Matrix (DTM) with bigrams and trigrams
dtm <- DocumentTermMatrix(corpus, control = list(tokenize = myTokenizer))
# Apply TF-IDF weighing
dtm_tfidf <- weightTfIdf(dtm)
# Apply TF-IDF weighing
dtm_tfidf <- weightTfIdf(dtm)
# Create a Corpus
corpus <- Corpus(VectorSource(stemmed_articles))
# Create a Corpus
corpus <- VCorpus(VectorSource(stemmed_articles))
# Define a custom tokenizer function for bigrams and trigrams
myTokenizer <- function(x) {
NGramTokenizer(x, Weka_control(min = 1, max = 2))
}
# Create a Document-Term Matrix (DTM) with bigrams and trigrams
dtm <- DocumentTermMatrix(corpus, control = list(tokenize = myTokenizer))
# Apply TF-IDF weighing
dtm_tfidf <- weightTfIdf(dtm)
# Label encoding
myData$labelnumber <- as.numeric(as.factor(myData$label))
# Merge the labels with the dtm_tfidf
dtm_tfidf <- cbind(dtm_tfidf, myData$labelnumber)
train_test_split <- function(dtm_tfidf, myData, partition_ratio) {
# Split the dataset into training and testing sets
trainIndex <- createDataPartition(myData$labelnumber, p = partition_ratio, list = FALSE, times = 1)
# Split the labels as well
train_labels <- myData$labelnumber[trainIndex]
test_labels  <- myData$labelnumber[-trainIndex]
# Split the dtm_tfidf
train <- dtm_tfidf[trainIndex,]
test <- dtm_tfidf[-trainIndex,]
# Prepare matrices suitable for Random Forest
train_df <- as.data.frame(as.matrix(train))
test_df <- as.data.frame(as.matrix(test))
colnames(train_df)[ncol(train_df)] <- "labelnumer"
colnames(test_df)[ncol(test_df)] <- "labelnumer"
# Rename the columns to use only alphanumeric characters and underscores
names(train_df) <- make.names(names(train_df), unique = TRUE)
names(test_df) <- make.names(names(test_df), unique = TRUE)
# Return the training and testing data frames and labels
return(list(train_df = train_df, test_df = test_df, train_labels = train_labels, test_labels = test_labels))
}
split_data <- train_test_split(dtm_tfidf, myData, 0.6)
train_df <- split_data$train_df
test_df <- split_data$test_df
train_labels <- split_data$train_labels
test_labels <- split_data$test_labels
# Train the model
model <- ranger(as.factor(train_labels) ~ ., data = train_df,
importance = 'impurity', num.trees = 500)
# Predict on the test set
predictions <- predict(model, test_df)
# Evaluate model performance
table(predictions$predictions, test_labels)
# Check accuracy
accuracy <- sum(predictions$predictions == test_labels) / length(test_labels)
accuracy
# Define the levels that should exist
all_levels <- 1:11 # Adjust this to the levels we expect to have
# Convert predictions and test_labels to factor and explicitly set the levels
predictions_factor <- factor(predictions$predictions, levels=all_levels)
test_labels_factor <- factor(test_labels, levels=all_levels)
# Compute the confusion matrix
cm <- confusionMatrix(predictions_factor, test_labels_factor)
# Print the confusion matrix
print(cm)
stemmed_articles <- stem_articles(articles_clean)
# Create a Corpus
corpus <- VCorpus(VectorSource(stemmed_articles))
# Define a custom tokenizer function
myTokenizer <- function(x) {
NGramTokenizer(x, Weka_control(min = 1, max = 1))
}
# Create a Document-Term Matrix (DTM) with bigrams and trigrams
dtm <- DocumentTermMatrix(corpus, control = list(tokenize = myTokenizer))
# Apply TF-IDF weighing
dtm_tfidf <- weightTfIdf(dtm)
# Label encoding
myData$labelnumber <- as.numeric(as.factor(myData$label))
# Merge the labels with the dtm_tfidf
dtm_tfidf <- cbind(dtm_tfidf, myData$labelnumber)
train_test_split <- function(dtm_tfidf, myData, partition_ratio) {
# Split the dataset into training and testing sets
trainIndex <- createDataPartition(myData$labelnumber, p = partition_ratio, list = FALSE, times = 1)
# Split the labels as well
train_labels <- myData$labelnumber[trainIndex]
test_labels  <- myData$labelnumber[-trainIndex]
# Split the dtm_tfidf
train <- dtm_tfidf[trainIndex,]
test <- dtm_tfidf[-trainIndex,]
# Prepare matrices suitable for Random Forest
train_df <- as.data.frame(as.matrix(train))
test_df <- as.data.frame(as.matrix(test))
colnames(train_df)[ncol(train_df)] <- "labelnumer"
colnames(test_df)[ncol(test_df)] <- "labelnumer"
# Rename the columns to use only alphanumeric characters and underscores
names(train_df) <- make.names(names(train_df), unique = TRUE)
names(test_df) <- make.names(names(test_df), unique = TRUE)
# Return the training and testing data frames and labels
return(list(train_df = train_df, test_df = test_df, train_labels = train_labels, test_labels = test_labels))
}
split_data <- train_test_split(dtm_tfidf, myData, 0.6)
train_df <- split_data$train_df
test_df <- split_data$test_df
train_labels <- split_data$train_labels
test_labels <- split_data$test_labels
# Train the model
model <- ranger(as.factor(train_labels) ~ ., data = train_df,
importance = 'impurity', num.trees = 500)
# Predict on the test set
predictions <- predict(model, test_df)
# Evaluate model performance
table(predictions$predictions, test_labels)
# Check accuracy
accuracy <- sum(predictions$predictions == test_labels) / length(test_labels)
# Define the levels that should exist
all_levels <- 1:11 # Adjust this to the levels we expect to have
accuracy
# Define the levels that should exist
all_levels <- 1:11 # Adjust this to the levels we expect to have
# Convert predictions and test_labels to factor and explicitly set the levels
predictions_factor <- factor(predictions$predictions, levels=all_levels)
test_labels_factor <- factor(test_labels, levels=all_levels)
# Compute the confusion matrix
cm <- confusionMatrix(predictions_factor, test_labels_factor)
# Print the confusion matrix
print(cm)
# Run a Bagging model
control <- trainControl(method = "cv", number = 2) # Changed method to 'cv' for cross-validation and number to 2 for 2-fold cross-validation, as it is computationally heavy.
model_bag <- caret::train(as.factor(labelnumer) ~ ., data=train_df, trControl=control, method="treebag")
predictions_bag <- predict(model_bag, newdata = test_df, type="raw")
# Compute the confusion matrix for bagging model
cm_bag <- confusionMatrix(predictions_bag, test_labels_factor)
# Print the confusion matrix
print(cm_bag)
source("C:/Users/Admin/Desktop/RR/RRProject/RR_project/textcorpus_reproduced.R", echo=TRUE)
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(echo = TRUE)
```{r include=FALSE}
# Install and load necessary packages
requiredPackages <- c(
"tm", "SnowballC", "slam", "topicmodels", "quanteda",
"caret", "e1071", "randomForest", "kernlab", "cluster",
"topicmodels", "LDAvis", "ggplot2", "rlang", "ranger", "Matrix",
"RWeka", "caret", "stringr", "tokenizers"
)
# Install and load necessary packages
requiredPackages <- c(
"tm", "SnowballC", "slam", "topicmodels", "quanteda",
"caret", "e1071", "randomForest", "kernlab", "cluster",
"topicmodels", "LDAvis", "ggplot2", "rlang", "ranger", "Matrix",
"RWeka", "caret", "stringr", "tokenizers"
)
# Install required packages if not already installed
invisible(sapply(requiredPackages, installPackage))
# Load the installed packages
lapply(requiredPackages, require, character.only = TRUE)
---
title: "TextCorpus"
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(echo = TRUE)
It generates a document-term matrix (dtm) using the DocumentTermMatrix function from the tm package. This matrix represents the frequency of terms (words) in the text documents.
It transforms the document-term matrix dtm into a term frequency-inverse document frequency (TF-IDF) matrix (dtm_tfidf) using the weightTfIdf function. TF-IDF is a numerical statistic that reflects the importance of a term in a document relative to a collection of documents.
It merges the labels from the data data frame with the TF-IDF matrix dtm_tfidf using the cbind function. This combines the label information with the corresponding document representation.
# Function to install package if not already installed
installPackage <- function(packageName) {
if (!(packageName %in% installed.packages())) {
install.packages(packageName)
}
}
# Install and load necessary packages
requiredPackages <- c(
"tm", "SnowballC", "slam", "topicmodels", "quanteda",
"caret", "e1071", "randomForest", "kernlab", "cluster",
"topicmodels", "LDAvis", "ggplot2", "rlang", "ranger", "Matrix",
"RWeka", "caret", "stringr", "tokenizers"
)
# Install required packages if not already installed
invisible(sapply(requiredPackages, installPackage))
# Load the installed packages
lapply(requiredPackages, require, character.only = TRUE)
set.seed(4545) #seed for the reproducibility
################## function to read txt and return data frame
multiTextFile <- function(directoryPath) {
# Get the list of file names in the directory
fileNames <- list.files(path = directoryPath, pattern = "\\.txt$", full.names = TRUE)
# Create an empty list to store the data frames
dataFrames <- list()
# Read each file and store it as a separate data frame
for (filePath in fileNames) {
data <- read.table(filePath, header = TRUE)  # Modify read function based on your file format
# Add the data frame to the list
dataFrames <- c(dataFrames, list(data))
}
# Merge the data frames into a single data frame
mergedData <- do.call(rbind, dataFrames)
names(mergedData) <- 'text'
mergedData <- mergedData[!duplicated(mergedData$text), ]
# Return the merged data frame
return(mergedData)
}
csvText <- function(file, textCol, labelCol) {
data <- read.csv(file)
data = data[,c(textCol,labelCol)]
names(data) = c('text','label')
names(data[,labelCol]) = 'label'
data <- data[!duplicated(data$text), ]
data$text = as.character(data$text)
data$label = as.factor(data$label)
return(data)
}
myData = csvText('fulldata-updated.csv', 'title', 'label')
assignLabels <- function(df,labels) {
df <- cbind(df,labels)
names(df)[2] = 'label'
return(df)
}
validLabels <- function(df) {
labelcount <- table(df$label)
repeated <- names(labelcount[labelcount > 1])
df <- df[df$label %in% repeated, ]
df <- droplevels(df)  # Drop unused levels if needed
rownames(df) <- seq_len(nrow(df))
return(df)
}
myData <- validLabels(myData)
cleanText <- function(data) {
data$text <- str_replace_all(data$text, "\n", " ")
data$text <- str_replace_all(data$text, "[0-9]+", "")
data$text <- str_replace_all(data$text, "[,\\!?/:;''()``’“-”—#]", "")
data$text <- str_replace_all(data$text, "[.]+", "")
data$text <- tolower(data$text)
data$text <- str_replace_all(data$text, "\\b\\w\\b", "")
data$text <- as.character(data$text)
return(data$text)
}
articles <- cleanText(myData)
remove_stopwords <- function(texts, stopwords) {
cleaned_texts <- lapply(texts, function(text) {
# Tokenize the text
tokens <- strsplit(tolower(text), "\\s+")
# Remove stopwords
tokens_clean <- tokens[[1]][!(tokens[[1]] %in% stopwords)]
# Join the cleaned tokens back into a text
cleaned_text <- paste(tokens_clean, collapse = " ")
# Return the cleaned text
return(cleaned_text)
})
return(cleaned_texts)
}
# Create custom stopwords
stopwordscustom <- read.csv('stp.csv', header = FALSE, col.names = c('word'))
stopwordscustom <- c(stopwordscustom$word, 'article', 'with')  # Add 'article' and 'with' to the custom stopwords list
articles_clean <- remove_stopwords(articles, stopwordscustom)
# Stemming function
stem_articles <- function(articles) {
stemmed_articles <- character(length(articles))
for (i in 1:length(articles)) {
words <- c()
for (word in articles[[i]]) {
stemmed_word <- SnowballC::wordStem(word, "porter")
words <- c(words, stemmed_word)
}
stemmed_articles[i] <- paste(words, collapse = " ")
}
return(stemmed_articles)
}
stemmed_articles <- stem_articles(articles_clean)
# Create a Corpus
corpus <- VCorpus(VectorSource(stemmed_articles))
# Define a custom tokenizer function
myTokenizer <- function(x) {
NGramTokenizer(x, Weka_control(min = 1, max = 1))
}
# Create a Document-Term Matrix (DTM) with bigrams and trigrams
dtm <- DocumentTermMatrix(corpus, control = list(tokenize = myTokenizer))
# Apply TF-IDF weighing
dtm_tfidf <- weightTfIdf(dtm)
# Label encoding
myData$labelnumber <- as.numeric(as.factor(myData$label))
# Merge the labels with the dtm_tfidf
dtm_tfidf <- cbind(dtm_tfidf, myData$labelnumber)
train_test_split <- function(dtm_tfidf, myData, partition_ratio) {
# Split the dataset into training and testing sets
trainIndex <- createDataPartition(myData$labelnumber, p = partition_ratio, list = FALSE, times = 1)
# Split the labels as well
train_labels <- myData$labelnumber[trainIndex]
test_labels  <- myData$labelnumber[-trainIndex]
# Split the dtm_tfidf
train <- dtm_tfidf[trainIndex,]
test <- dtm_tfidf[-trainIndex,]
# Prepare matrices suitable for Random Forest
train_df <- as.data.frame(as.matrix(train))
test_df <- as.data.frame(as.matrix(test))
colnames(train_df)[ncol(train_df)] <- "labelnumer"
colnames(test_df)[ncol(test_df)] <- "labelnumer"
# Rename the columns to use only alphanumeric characters and underscores
names(train_df) <- make.names(names(train_df), unique = TRUE)
names(test_df) <- make.names(names(test_df), unique = TRUE)
# Return the training and testing data frames and labels
return(list(train_df = train_df, test_df = test_df, train_labels = train_labels, test_labels = test_labels))
}
split_data <- train_test_split(dtm_tfidf, myData, 0.6)
train_df <- split_data$train_df
test_df <- split_data$test_df
train_labels <- split_data$train_labels
test_labels <- split_data$test_labels
# Train the model
model <- ranger(as.factor(train_labels) ~ ., data = train_df,
importance = 'impurity', num.trees = 500)
# Predict on the test set
predictions <- predict(model, test_df)
# Evaluate model performance
table(predictions$predictions, test_labels)
# Check accuracy
accuracy <- sum(predictions$predictions == test_labels) / length(test_labels)
# Define the levels that should exist
all_levels <- 1:11 # Adjust this to the levels we expect to have
# Convert predictions and test_labels to factor and explicitly set the levels
predictions_factor <- factor(predictions$predictions, levels=all_levels)
test_labels_factor <- factor(test_labels, levels=all_levels)
# Compute the confusion matrix
cm <- confusionMatrix(predictions_factor, test_labels_factor)
# Print the confusion matrix
print(cm)
plot_lda_topics <- function(articles, k, num_top_terms = 10, title = "Top Terms in Each LDA Topic", min_ngram=1, max_ngram=2) {
# Create a Corpus
corpus <- VCorpus(VectorSource(stemmed_articles))
# Define a custom tokenizer function for bigrams and trigrams
myTokenizer <- function(x) {
NGramTokenizer(x, Weka_control(min = min_ngram, max = max_ngram))
}
# Create a Document-Term Matrix (DTM) with bigrams and trigrams
dtm <- DocumentTermMatrix(corpus, control = list(tokenize = myTokenizer))
# Run LDA model
lda <- LDA(dtm, k = k, control = list(seed = 3434))
# Get the top terms for each topic
topics <- tidy(lda, matrix = "beta")
top_terms <- topics %>%
group_by(topic) %>%
top_n(num_top_terms, beta) %>%
ungroup() %>%
arrange(topic, -beta)
# Plot the LDA topics
plot_lda <- top_terms %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(beta, term, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
theme_minimal() +
labs(title = title, x = "Beta", y = "")
print(plot_lda)
}
plot_lda_topics(articles = articles_clean, k = 10, num_top_terms = 10,
title = paste("Top ",k, "Terms in Each LDA Topic"), min_ngram = 1, max_ngram = 2)
plot_lda_topics <- function(articles, k, num_top_terms = 10, min_ngram=1, max_ngram=2) {
# Create a Corpus
corpus <- VCorpus(VectorSource(stemmed_articles))
# Define a custom tokenizer function for bigrams and trigrams
myTokenizer <- function(x) {
NGramTokenizer(x, Weka_control(min = min_ngram, max = max_ngram))
}
# Create a Document-Term Matrix (DTM) with bigrams and trigrams
dtm <- DocumentTermMatrix(corpus, control = list(tokenize = myTokenizer))
# Run LDA model
lda <- LDA(dtm, k = k, control = list(seed = 3434))
# Get the top terms for each topic
topics <- tidy(lda, matrix = "beta")
top_terms <- topics %>%
group_by(topic) %>%
top_n(num_top_terms, beta) %>%
ungroup() %>%
arrange(topic, -beta)
# Plot the LDA topics
plot_lda <- top_terms %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(beta, term, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
theme_minimal() +
labs(title = paste("Top ",k, "Terms in Each LDA Topic"), x = "Beta", y = "")
print(plot_lda)
}
plot_lda_topics(articles = articles_clean, k = 10, num_top_terms = 10,
min_ngram = 1, max_ngram = 2)
# Plot the LDA topics
plot_lda <- top_terms %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(beta, term, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
theme_minimal() +
labs(title = paste("Top",k, "Terms in Each LDA Topic"), x = "Beta", y = "")
plot_lda_topics <- function(articles, k, num_top_terms = 10, min_ngram=1, max_ngram=2) {
# Create a Corpus
corpus <- VCorpus(VectorSource(stemmed_articles))
# Define a custom tokenizer function for bigrams and trigrams
myTokenizer <- function(x) {
NGramTokenizer(x, Weka_control(min = min_ngram, max = max_ngram))
}
# Create a Document-Term Matrix (DTM) with bigrams and trigrams
dtm <- DocumentTermMatrix(corpus, control = list(tokenize = myTokenizer))
# Run LDA model
lda <- LDA(dtm, k = k, control = list(seed = 3434))
# Get the top terms for each topic
topics <- tidy(lda, matrix = "beta")
top_terms <- topics %>%
group_by(topic) %>%
top_n(num_top_terms, beta) %>%
ungroup() %>%
arrange(topic, -beta)
# Plot the LDA topics
plot_lda <- top_terms %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(beta, term, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
theme_minimal() +
labs(title = paste("Top",k, "Terms in Each LDA Topic"), x = "Beta", y = "")
print(plot_lda)
}
plot_lda_topics(articles = articles_clean, k = 10, num_top_terms = 10,
min_ngram = 1, max_ngram = 2)
# Word cloud
library(wordcloud)
library(RColorBrewer)
# Install and load necessary packages
requiredPackages <- c(
"tm", "SnowballC", "slam", "topicmodels", "quanteda",
"caret", "e1071", "randomForest", "kernlab", "cluster",
"topicmodels", "LDAvis", "ggplot2", "rlang", "ranger", "Matrix",
"RWeka", "caret", "stringr", "tokenizers", "wordcloud", "RColorBrewer"
)
# Install required packages if not already installed
invisible(sapply(requiredPackages, installPackage))
# Load the installed packages
lapply(requiredPackages, require, character.only = TRUE)
# Word cloud
# Function to create a word cloud from a document term matrix
createWordCloud <- function(dtm) {
# Get the word frequencies from the document term matrix
word_frequencies <- colSums(as.matrix(dtm))
# Sort the word frequencies in decreasing order
word_frequencies <- sort(word_frequencies, decreasing=TRUE)
# Create a data frame with words and their frequencies
df <- data.frame(word=names(word_frequencies), freq=word_frequencies)
# Generate the word cloud
wordcloud(words = df$word, freq = df$freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
}
# Word cloud
# Function to create a word cloud from a document term matrix
createWordCloud <- function(cleaned_articles, min_ngram=1, max_ngram=1) {
# Define tokenizing function inside a function again for ngram control
myTokenizer <- function(x) {
NGramTokenizer(x, Weka_control(min = min_ngram, max = max_ngram))
}
# Creat document term matrix from cleaned texts but not stemmed ones.
corpus <- VCorpus(VectorSource(cleaned_articles))
# Create a Document-Term Matrix (DTM)
dtm <- DocumentTermMatrix(corpus, control = list(tokenize = myTokenizer))
# Get the word frequencies from the document term matrix
word_frequencies <- colSums(as.matrix(dtm))
# Sort the word frequencies in decreasing order
word_frequencies <- sort(word_frequencies, decreasing=TRUE)
# Create a data frame with words and their frequencies
df <- data.frame(word=names(word_frequencies), freq=word_frequencies)
# Generate the word cloud
wordcloud(words = df$word, freq = df$freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
}
# Call the function to create the word cloud
createWordCloud(articles_clean, 1, 2)
install.packages("renv")
library(renv)
renv::init()
library(quarto)
install.packages('quarto')
install.packages('bslib')
chooseCRANmirror()
install.packages("bslib")
>>>>>>> 6b347d2151b0b2c0e7f0e95b376b6b4b8696a701
