library(stringr)
library(tokenizers)
library(SnowballC)
fulldata <- read.csv("fulldata-updated.csv")
fulldata <- fulldata[!duplicated(fulldata$title), ]
fulldata <- fulldata[order(rownames(fulldata)), ]
fulldata <- as.data.frame(lapply(fulldata, type.convert))
rownames(fulldata) <- NULL
source("C:/Users/Admin/Desktop/RR/RRProject/RR_project/textcorpus_reproduced.R", echo=TRUE)
# Clean text
articles <- str_replace_all(fulldata$article, "\n", " ")
articles <- str_replace_all(articles, "[0-9]+", "")
articles <- str_replace_all(articles, "[,\\!?/:;''()``’“-”—#]", "")
articles <- str_replace_all(articles, "[.]+", "")
articles <- tolower(articles)
articles <- str_replace_all(articles, "\\b\\w\\b", "")
articles <- as.character(articles)
# Tokenizer
articles <- sapply(articles, function(x) tokenizers::tokenize_words(x))
articles <- as.list(articles)
stemmed_articles <- list()
for (i in 1:length(articles)) {
words <- c()
for (word in articles[[i]]) {
stemmed_word <- SnowballC::wordStem(word, "english")
words <- c(words, stemmed_word)
}
stemmed_articles[[i]] <- words
}
stemmed_articles <- as.list(stemmed_articles)
#
stopwordscustom <- read.csv('stp.csv', header = FALSE, col.names = c('word'))
stopwordscustom <- as.character(stopwordscustom$word)
# Install and load necessary packages
#install.packages(c("tm", "SnowballC", "slam", "topicmodels", "quanteda", "caret", "e1071", "randomForest", "kernlab", "cluster", "topicmodels", "LDAvis", "ggplot2"))
library(tm)
library(SnowballC)
# Install and load necessary packages
install.packages(c("tm", "SnowballC", "slam", "topicmodels", "quanteda", "caret", "e1071", "randomForest", "kernlab", "cluster", "topicmodels", "LDAvis", "ggplot2"))
install.packages(c("tm", "SnowballC", "slam", "topicmodels", "quanteda", "caret", "e1071", "randomForest", "kernlab", "cluster", "topicmodels", "LDAvis", "ggplot2"))
library(tm)
library(SnowballC)
library(slam)
library(topicmodels)
library(quanteda)
library(caret)
library(ggplot2)
# Remove custom stopwords
articles <- lapply(articles, function(x) x[!x %in% stopwordscustom])
# Generate a document-term matrix
dtm <- DocumentTermMatrix(Corpus(VectorSource(articles)))
# Transform to a term frequency-inverse document frequency (TF-IDF) matrix
dtm_tfidf <- weightTfIdf(dtm)
# Label encoding
fulldata$labelnumber <- as.numeric(as.factor(fulldata$label))
# Merge the labels with the dtm
dtm_tfidf <- cbind(dtm_tfidf, fulldata$labelnumber)
# Split the dataset into training and testing sets
set.seed(4545)
trainIndex <- createDataPartition(fulldata$labelnumber, p = .6, list = FALSE, times = 1)
library(caret)
# Install and load necessary packages
install.packages(c("tm", "SnowballC", "slam", "topicmodels", "quanteda", "caret", "e1071", "randomForest", "kernlab", "cluster", "topicmodels", "LDAvis", "ggplot2", 'rlang'))
install.packages(c("tm", "SnowballC", "slam", "topicmodels", "quanteda", "caret", "e1071", "randomForest", "kernlab", "cluster", "topicmodels", "LDAvis", "ggplot2", "rlang"))
install.packages(c("tm", "SnowballC", "slam", "topicmodels", "quanteda", "caret", "e1071", "randomForest", "kernlab", "cluster", "topicmodels", "LDAvis", "ggplot2", "rlang"))
# Each comment is a .ipynb cell
library(stringr)
library(tokenizers)
library(SnowballC)
# Load data
fulldata <- read.csv("fulldata-updated.csv")
fulldata <- fulldata[!duplicated(fulldata$title), ]
fulldata <- fulldata[order(rownames(fulldata)), ]
fulldata <- as.data.frame(lapply(fulldata, type.convert))
rownames(fulldata) <- NULL
#
labelcount <- table(fulldata$label)
repeated <- names(labelcount[labelcount > 1])
fulldata <- fulldata[fulldata$label %in% repeated, ]
fulldata <- droplevels(fulldata)  # Drop unused levels if needed
rownames(fulldata) <- seq_len(nrow(fulldata))
#
fulldata$date <- as.POSIXct(fulldata$date, format = "%Y-%m-%d %H:%M:%S")
fulldata$label <- as.factor(fulldata$label)
month_stats <- summary(as.numeric(format(fulldata$date, "%m")))
fulldata$article <- as.character(fulldata$article)
# Clean text
articles <- str_replace_all(fulldata$article, "\n", " ")
articles <- str_replace_all(articles, "[0-9]+", "")
articles <- str_replace_all(articles, "[,\\!?/:;''()``’“-”—#]", "")
articles <- str_replace_all(articles, "[.]+", "")
articles <- tolower(articles)
articles <- str_replace_all(articles, "\\b\\w\\b", "")
articles <- as.character(articles)
# Tokenizer
articles <- sapply(articles, function(x) tokenizers::tokenize_words(x))
articles <- as.list(articles)
# PorterStemmer
stemmed_articles <- list()
for (i in 1:length(articles)) {
words <- c()
for (word in articles[[i]]) {
stemmed_word <- SnowballC::wordStem(word, "english")
words <- c(words, stemmed_word)
}
stemmed_articles[[i]] <- words
}
stemmed_articles <- as.list(stemmed_articles)
#
stopwordscustom <- read.csv('stp.csv', header = FALSE, col.names = c('word'))
stopwordscustom <- as.character(stopwordscustom$word)
# Install and load necessary packages
#install.packages(c("tm", "SnowballC", "slam", "topicmodels", "quanteda", "caret", "e1071", "randomForest", "kernlab", "cluster", "topicmodels", "LDAvis", "ggplot2", 'rlang'))
library(tm)
library(SnowballC)
library(slam)
library(topicmodels)
library(quanteda)
library(caret)
library(e1071)
library(randomForest)
library(kernlab)
library(cluster)
library(topicmodels)
library(LDAvis)
library(ggplot2)
# Remove custom stopwords
articles <- lapply(articles, function(x) x[!x %in% stopwordscustom])
# Generate a document-term matrix
dtm <- DocumentTermMatrix(Corpus(VectorSource(articles)))
# Transform to a term frequency-inverse document frequency (TF-IDF) matrix
dtm_tfidf <- weightTfIdf(dtm)
# Label encoding
fulldata$labelnumber <- as.numeric(as.factor(fulldata$label))
# Merge the labels with the dtm
dtm_tfidf <- cbind(dtm_tfidf, fulldata$labelnumber)
# Split the dataset into training and testing sets
set.seed(4545)
trainIndex <- createDataPartition(fulldata$labelnumber, p = .6, list = FALSE, times = 1)
train <- dtm_tfidf[ trainIndex,]
test  <- dtm_tfidf[-trainIndex,]
# Run a Extra Trees model
model <- randomForest(as.factor(labelnumber) ~ ., data = train, ntree = 100, mtry = 2, importance = TRUE)
train_df <- as.data.frame(as.matrix(train))
# Run a Extra Trees model
model <- randomForest(as.factor(labelnumber) ~ ., data = train, ntree = 100, mtry = 2, importance = TRUE)
str(train)
# Convert the sparse matrix to a dense matrix
train_dense <- as.matrix(train)
# Convert the dense matrix to a data frame
train_df <- as.data.frame(train_dense)
# Run a Extra Trees model
model <- randomForest(as.factor(labelnumber) ~ ., data = train, ntree = 100, mtry = 2, importance = TRUE)
# Run a Extra Trees model
model <- randomForest(as.factor(labelnumber) ~ ., data = train_df, ntree = 100, mtry = 2, importance = TRUE)
# Run a Extra Trees model
model <- randomForest(as.factor(fulldata$labelnumber) ~ ., data = train_df, ntree = 100, mtry = 2, importance = TRUE)
# Add 'fulldata$labelnumber' to 'train_df'
train_df$labelnumber <- fulldata$labelnumber
model <- randomForest(as.factor(labelnumber) ~ ., data = train_df, ntree = 100, mtry = 2, importance = TRUE)
# Add 'fulldata$labelnumber' to 'train_df'
train_df$labelnumber <- fulldata$labelnumber
# Assuming that 'fulldata' is a dataframe that includes the feature vectors and labels
set.seed(123) # for reproducibility
training_indices <- caret::createDataPartition(fulldata$labelnumber, p = 0.6, list = FALSE)
train_df <- fulldata[training_indices, ]
# Add 'fulldata$labelnumber' to 'train_df'
train_df$labelnumber <- fulldata$labelnumber
View(train)
articles
dtm
dtm_tfidf
fulldata$labelnumber <- as.numeric(as.factor(fulldata$label))
fulldata$labelnumber
dtm_tfidf <- cbind(dtm_tfidf, fulldata$labelnumber)
dtm_tfidf
trainIndex
train <- dtm_tfidf[ trainIndex,]
test  <- dtm_tfidf[-trainIndex,]
# Run a Extra Trees model
model <- randomForest(as.factor(fulldata$labelnumber) ~ ., data = train, ntree = 100, mtry = 2, importance = TRUE)
# Convert the sparse matrix to a dense matrix
train_dense <- as.matrix(train)
# Convert the dense matrix to a data frame
train_df <- as.data.frame(train_dense)
train_df
train
# Run a Extra Trees model
model <- randomForest(as.factor(fulldata$labelnumber) ~ ., data = train, ntree = 100, mtry = 2, importance = TRUE)
# Run a Extra Trees model
model <- randomForest(as.factor(fulldata$labelnumber) ~ ., data = train_df, ntree = 100, mtry = 2, importance = TRUE)
# Run a Extra Trees model
model <- randomForest(as.factor(train_df$labelnumber) ~ ., data = train_df, ntree = 100, mtry = 2, importance = TRUE)
# Split the dataset into training and testing sets
set.seed(4545)
trainIndex <- createDataPartition(fulldata$labelnumber, p = .6, list = FALSE, times = 1)
# Split the labels as well
train_labels <- fulldata$labelnumber[trainIndex]
test_labels  <- fulldata$labelnumber[-trainIndex]
# Split the dtm_tfidf
train <- dtm_tfidf[trainIndex,]
test  <- dtm_tfidf[-trainIndex,]
# Convert the sparse matrix to a dense matrix
train_dense <- as.matrix(train)
# Convert the dense matrix to a data frame
train_df <- as.data.frame(train_dense)
# Now add the labels
train_df$labelnumber <- train_labels
# Run a Extra Trees model
model <- randomForest(as.factor(train_df$labelnumber) ~ ., data = train_df, ntree = 100, mtry = 2, importance = TRUE)
# Clean up column names
names(train_df) <- gsub('[^A-Za-z0-9_]', '', names(train_df))
# Run a Extra Trees model
model <- randomForest(as.factor(train_df$labelnumber) ~ ., data = train_df, ntree = 100, mtry = 2, importance = TRUE)
# Make column names unique
names(train_df) <- make.names(names(train_df), unique = TRUE)
# Run a Extra Trees model
model <- randomForest(as.factor(train_df$labelnumber) ~ ., data = train_df, ntree = 100, mtry = 2, importance = TRUE)
# Predict on the test set
predictions <- predict(model, newdata = test)
# Convert the sparse matrix to a dense matrix
test_dense <- as.matrix(test)
# Convert the dense matrix to a data frame
test_df <- as.data.frame(test_dense)
# Predict on the test set
predictions <- predict(model, newdata = test_df)
# Make column names unique
names(test_df) <- make.names(names(test_df), unique = TRUE)
# Predict on the test set
predictions <- predict(model, newdata = test_df)
# Print classification report
print(confusionMatrix(predictions, test$labelnumber))
# Predict on the test set
predictions <- predict(model, newdata = test_df)
# Ensure that the test data has the same columns as the train data
test_df <- test_df[, names(train_df), drop = FALSE]
# Predict on the test set
predictions <- predict(model, newdata = test_df)
test_df
# Ensure that the test data has the same columns as the train data
missing_cols <- setdiff(names(train_df), names(test_df))
for (col in missing_cols) {
test_df[[col]] <- 0
}
# Now add the labels
test_df$labelnumber <- test_labels
# Make column names unique
names(test_df) <- make.names(names(test_df), unique = TRUE)
# Predict on the test set
predictions <- predict(model, newdata = test_df)
train_df
