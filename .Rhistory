# Install and load necessary packages
#install.packages(c("tm", "SnowballC", "slam", "topicmodels", "quanteda", "caret", "e1071", "randomForest", "kernlab", "cluster", "topicmodels", "LDAvis", "ggplot2"))
library(tm)
library(SnowballC)
# Install and load necessary packages
install.packages(c("tm", "SnowballC", "slam", "topicmodels", "quanteda", "caret", "e1071", "randomForest", "kernlab", "cluster", "topicmodels", "LDAvis", "ggplot2"))
install.packages(c("tm", "SnowballC", "slam", "topicmodels", "quanteda", "caret", "e1071", "randomForest", "kernlab", "cluster", "topicmodels", "LDAvis", "ggplot2"))
library(tm)
library(SnowballC)
library(slam)
library(topicmodels)
library(quanteda)
library(caret)
library(ggplot2)
# Remove custom stopwords
articles <- lapply(articles, function(x) x[!x %in% stopwordscustom])
# Generate a document-term matrix
dtm <- DocumentTermMatrix(Corpus(VectorSource(articles)))
# Transform to a term frequency-inverse document frequency (TF-IDF) matrix
dtm_tfidf <- weightTfIdf(dtm)
# Label encoding
fulldata$labelnumber <- as.numeric(as.factor(fulldata$label))
# Merge the labels with the dtm
dtm_tfidf <- cbind(dtm_tfidf, fulldata$labelnumber)
# Split the dataset into training and testing sets
set.seed(4545)
trainIndex <- createDataPartition(fulldata$labelnumber, p = .6, list = FALSE, times = 1)
library(caret)
# Install and load necessary packages
install.packages(c("tm", "SnowballC", "slam", "topicmodels", "quanteda", "caret", "e1071", "randomForest", "kernlab", "cluster", "topicmodels", "LDAvis", "ggplot2", 'rlang'))
install.packages(c("tm", "SnowballC", "slam", "topicmodels", "quanteda", "caret", "e1071", "randomForest", "kernlab", "cluster", "topicmodels", "LDAvis", "ggplot2", "rlang"))
install.packages(c("tm", "SnowballC", "slam", "topicmodels", "quanteda", "caret", "e1071", "randomForest", "kernlab", "cluster", "topicmodels", "LDAvis", "ggplot2", "rlang"))
# Each comment is a .ipynb cell
library(stringr)
library(tokenizers)
library(SnowballC)
# Load data
fulldata <- read.csv("fulldata-updated.csv")
fulldata <- fulldata[!duplicated(fulldata$title), ]
fulldata <- fulldata[order(rownames(fulldata)), ]
fulldata <- as.data.frame(lapply(fulldata, type.convert))
rownames(fulldata) <- NULL
#
labelcount <- table(fulldata$label)
repeated <- names(labelcount[labelcount > 1])
fulldata <- fulldata[fulldata$label %in% repeated, ]
fulldata <- droplevels(fulldata)  # Drop unused levels if needed
rownames(fulldata) <- seq_len(nrow(fulldata))
#
fulldata$date <- as.POSIXct(fulldata$date, format = "%Y-%m-%d %H:%M:%S")
fulldata$label <- as.factor(fulldata$label)
month_stats <- summary(as.numeric(format(fulldata$date, "%m")))
fulldata$article <- as.character(fulldata$article)
# Clean text
articles <- str_replace_all(fulldata$article, "\n", " ")
articles <- str_replace_all(articles, "[0-9]+", "")
articles <- str_replace_all(articles, "[,\\!?/:;''()``’“-”—#]", "")
articles <- str_replace_all(articles, "[.]+", "")
articles <- tolower(articles)
articles <- str_replace_all(articles, "\\b\\w\\b", "")
articles <- as.character(articles)
# Tokenizer
articles <- sapply(articles, function(x) tokenizers::tokenize_words(x))
articles <- as.list(articles)
# PorterStemmer
stemmed_articles <- list()
for (i in 1:length(articles)) {
words <- c()
for (word in articles[[i]]) {
stemmed_word <- SnowballC::wordStem(word, "english")
words <- c(words, stemmed_word)
}
stemmed_articles[[i]] <- words
}
stemmed_articles <- as.list(stemmed_articles)
#
stopwordscustom <- read.csv('stp.csv', header = FALSE, col.names = c('word'))
stopwordscustom <- as.character(stopwordscustom$word)
# Install and load necessary packages
#install.packages(c("tm", "SnowballC", "slam", "topicmodels", "quanteda", "caret", "e1071", "randomForest", "kernlab", "cluster", "topicmodels", "LDAvis", "ggplot2", 'rlang'))
library(tm)
library(SnowballC)
library(slam)
library(topicmodels)
library(quanteda)
library(caret)
library(e1071)
library(randomForest)
library(kernlab)
library(cluster)
library(topicmodels)
library(LDAvis)
library(ggplot2)
# Remove custom stopwords
articles <- lapply(articles, function(x) x[!x %in% stopwordscustom])
# Generate a document-term matrix
dtm <- DocumentTermMatrix(Corpus(VectorSource(articles)))
# Transform to a term frequency-inverse document frequency (TF-IDF) matrix
dtm_tfidf <- weightTfIdf(dtm)
# Label encoding
fulldata$labelnumber <- as.numeric(as.factor(fulldata$label))
# Merge the labels with the dtm
dtm_tfidf <- cbind(dtm_tfidf, fulldata$labelnumber)
# Split the dataset into training and testing sets
set.seed(4545)
trainIndex <- createDataPartition(fulldata$labelnumber, p = .6, list = FALSE, times = 1)
train <- dtm_tfidf[ trainIndex,]
test  <- dtm_tfidf[-trainIndex,]
# Run a Extra Trees model
model <- randomForest(as.factor(labelnumber) ~ ., data = train, ntree = 100, mtry = 2, importance = TRUE)
train_df <- as.data.frame(as.matrix(train))
# Run a Extra Trees model
model <- randomForest(as.factor(labelnumber) ~ ., data = train, ntree = 100, mtry = 2, importance = TRUE)
str(train)
# Convert the sparse matrix to a dense matrix
train_dense <- as.matrix(train)
# Convert the dense matrix to a data frame
train_df <- as.data.frame(train_dense)
# Run a Extra Trees model
model <- randomForest(as.factor(labelnumber) ~ ., data = train, ntree = 100, mtry = 2, importance = TRUE)
# Run a Extra Trees model
model <- randomForest(as.factor(labelnumber) ~ ., data = train_df, ntree = 100, mtry = 2, importance = TRUE)
# Run a Extra Trees model
model <- randomForest(as.factor(fulldata$labelnumber) ~ ., data = train_df, ntree = 100, mtry = 2, importance = TRUE)
# Add 'fulldata$labelnumber' to 'train_df'
train_df$labelnumber <- fulldata$labelnumber
model <- randomForest(as.factor(labelnumber) ~ ., data = train_df, ntree = 100, mtry = 2, importance = TRUE)
# Add 'fulldata$labelnumber' to 'train_df'
train_df$labelnumber <- fulldata$labelnumber
# Assuming that 'fulldata' is a dataframe that includes the feature vectors and labels
set.seed(123) # for reproducibility
training_indices <- caret::createDataPartition(fulldata$labelnumber, p = 0.6, list = FALSE)
train_df <- fulldata[training_indices, ]
# Add 'fulldata$labelnumber' to 'train_df'
train_df$labelnumber <- fulldata$labelnumber
View(train)
articles
dtm
dtm_tfidf
fulldata$labelnumber <- as.numeric(as.factor(fulldata$label))
fulldata$labelnumber
dtm_tfidf <- cbind(dtm_tfidf, fulldata$labelnumber)
dtm_tfidf
trainIndex
train <- dtm_tfidf[ trainIndex,]
test  <- dtm_tfidf[-trainIndex,]
# Run a Extra Trees model
model <- randomForest(as.factor(fulldata$labelnumber) ~ ., data = train, ntree = 100, mtry = 2, importance = TRUE)
# Convert the sparse matrix to a dense matrix
train_dense <- as.matrix(train)
# Convert the dense matrix to a data frame
train_df <- as.data.frame(train_dense)
train_df
train
# Run a Extra Trees model
model <- randomForest(as.factor(fulldata$labelnumber) ~ ., data = train, ntree = 100, mtry = 2, importance = TRUE)
# Run a Extra Trees model
model <- randomForest(as.factor(fulldata$labelnumber) ~ ., data = train_df, ntree = 100, mtry = 2, importance = TRUE)
# Run a Extra Trees model
model <- randomForest(as.factor(train_df$labelnumber) ~ ., data = train_df, ntree = 100, mtry = 2, importance = TRUE)
# Split the dataset into training and testing sets
set.seed(4545)
trainIndex <- createDataPartition(fulldata$labelnumber, p = .6, list = FALSE, times = 1)
# Split the labels as well
train_labels <- fulldata$labelnumber[trainIndex]
test_labels  <- fulldata$labelnumber[-trainIndex]
# Split the dtm_tfidf
train <- dtm_tfidf[trainIndex,]
test  <- dtm_tfidf[-trainIndex,]
# Convert the sparse matrix to a dense matrix
train_dense <- as.matrix(train)
# Convert the dense matrix to a data frame
train_df <- as.data.frame(train_dense)
# Now add the labels
train_df$labelnumber <- train_labels
# Run a Extra Trees model
model <- randomForest(as.factor(train_df$labelnumber) ~ ., data = train_df, ntree = 100, mtry = 2, importance = TRUE)
# Clean up column names
names(train_df) <- gsub('[^A-Za-z0-9_]', '', names(train_df))
# Run a Extra Trees model
model <- randomForest(as.factor(train_df$labelnumber) ~ ., data = train_df, ntree = 100, mtry = 2, importance = TRUE)
# Make column names unique
names(train_df) <- make.names(names(train_df), unique = TRUE)
# Run a Extra Trees model
model <- randomForest(as.factor(train_df$labelnumber) ~ ., data = train_df, ntree = 100, mtry = 2, importance = TRUE)
# Predict on the test set
predictions <- predict(model, newdata = test)
# Convert the sparse matrix to a dense matrix
test_dense <- as.matrix(test)
# Convert the dense matrix to a data frame
test_df <- as.data.frame(test_dense)
# Predict on the test set
predictions <- predict(model, newdata = test_df)
# Make column names unique
names(test_df) <- make.names(names(test_df), unique = TRUE)
# Predict on the test set
predictions <- predict(model, newdata = test_df)
# Print classification report
print(confusionMatrix(predictions, test$labelnumber))
# Predict on the test set
predictions <- predict(model, newdata = test_df)
# Ensure that the test data has the same columns as the train data
test_df <- test_df[, names(train_df), drop = FALSE]
# Predict on the test set
predictions <- predict(model, newdata = test_df)
test_df
# Ensure that the test data has the same columns as the train data
missing_cols <- setdiff(names(train_df), names(test_df))
for (col in missing_cols) {
test_df[[col]] <- 0
}
# Now add the labels
test_df$labelnumber <- test_labels
# Make column names unique
names(test_df) <- make.names(names(test_df), unique = TRUE)
# Predict on the test set
predictions <- predict(model, newdata = test_df)
train_df
library(tm)
library(slam)
library(quanteda)
library(caret)
library(e1071)
library(randomForest)
library(kernlab)
library(cluster)
library(topicmodels)
library(LDAvis)
library(ggplot2)
library(stringr)
library(tokenizers)
library(SnowballC)
# Load data
fulldata <- read.csv("fulldata-updated.csv")
fulldata <- fulldata[!duplicated(fulldata$title), ]
fulldata <- fulldata[order(rownames(fulldata)), ]
fulldata <- as.data.frame(lapply(fulldata, type.convert))
rownames(fulldata) <- NULL
#
labelcount <- table(fulldata$label)
repeated <- names(labelcount[labelcount > 1])
fulldata <- fulldata[fulldata$label %in% repeated, ]
fulldata <- droplevels(fulldata)  # Drop unused levels if needed
rownames(fulldata) <- seq_len(nrow(fulldata))
#
fulldata$date <- as.POSIXct(fulldata$date, format = "%Y-%m-%d %H:%M:%S")
fulldata$label <- as.factor(fulldata$label)
month_stats <- summary(as.numeric(format(fulldata$date, "%m")))
fulldata$article <- as.character(fulldata$article)
# Clean text
articles <- str_replace_all(fulldata$article, "\n", " ")
articles <- str_replace_all(articles, "[0-9]+", "")
articles <- str_replace_all(articles, "[,\\!?/:;''()``’“-”—#]", "")
articles <- str_replace_all(articles, "[.]+", "")
articles <- tolower(articles)
articles <- str_replace_all(articles, "\\b\\w\\b", "")
articles <- as.character(articles)
# Tokenizer
articles <- sapply(articles, function(x) tokenizers::tokenize_words(x))
articles <- as.list(articles)
# PorterStemmer
stemmed_articles <- list()
for (i in 1:length(articles)) {
words <- c()
for (word in articles[[i]]) {
stemmed_word <- SnowballC::wordStem(word, "english")
words <- c(words, stemmed_word)
}
stemmed_articles[[i]] <- words
}
stemmed_articles <- as.list(stemmed_articles)
#
stopwordscustom <- read.csv('stp.csv', header = FALSE, col.names = c('word'))
stopwordscustom <- as.character(stopwordscustom$word)
View(fulldata)
View(articles)
# Convert the sparse matrix to a dense matrix
train_dense <- as.matrix(train)
# Convert the sparse matrix to a dense matrix
train_dense <- as.matrix(train)
# Generate a document-term matrix
dtm <- DocumentTermMatrix(Corpus(VectorSource(articles)))
# Transform to a term frequency-inverse document frequency (TF-IDF) matrix
dtm_tfidf <- weightTfIdf(dtm)
# Label encoding
fulldata$labelnumber <- as.numeric(as.factor(fulldata$label))
# Merge the labels with the dtm
dtm_tfidf <- cbind(dtm_tfidf, fulldata$labelnumber)
# Split the dataset into training and testing sets
set.seed(4545) #seed for the reproducibility
trainIndex <- createDataPartition(fulldata$labelnumber, p = .6, list = FALSE, times = 1)
# Split the labels as well
train_labels <- fulldata$labelnumber[trainIndex]
test_labels  <- fulldata$labelnumber[-trainIndex]
# Split the dtm_tfidf
train <- dtm_tfidf[trainIndex,]
test  <- dtm_tfidf[-trainIndex,]
# Convert the sparse matrix to a dense matrix
train_dense <- as.matrix(train)
# Convert the dense matrix to a data frame
train_df <- as.data.frame(train_dense)
# Now add the labels
train_df$labelnumber <- train_labels
# Make column names unique
names(train_df) <- make.names(names(train_df), unique = TRUE)
# Run a Extra Trees model
model <- randomForest(as.factor(train_df$labelnumber) ~ ., data = train_df, ntree = 100, mtry = 2, importance = TRUE)
train
test
train_dense
train_df
train
### ATTEMPT TO FIX PROBLEMS
library(Matrix)
library(ranger)
# Each comment is a .ipynb cell
# Install and load necessary packages
#install.packages(c("tm", "SnowballC", "slam", "topicmodels", "quanteda", "caret", "e1071", "randomForest", "kernlab", "cluster", "topicmodels", "LDAvis", "ggplot2", 'rlang'))
install.packages('ranger')
# Each comment is a .ipynb cell
# Install and load necessary packages
#install.packages(c("tm", "SnowballC", "slam", "topicmodels", "quanteda", "caret", "e1071", "randomForest", "kernlab", "cluster", "topicmodels", "LDAvis", "ggplot2", 'rlang'))
install.packages('ranger')
# Each comment is a .ipynb cell
# Install and load necessary packages
#install.packages(c("tm", "SnowballC", "slam", "topicmodels", "quanteda", "caret", "e1071", "randomForest", "kernlab", "cluster", "topicmodels", "LDAvis", "ggplot2", 'rlang'))
install.packages('ranger')
# Each comment is a .ipynb cell
# Install and load necessary packages
install.packages(c("tm", "SnowballC", "slam", "topicmodels", "quanteda", "caret",
"e1071", "randomForest", "kernlab", "cluster", "topicmodels",
"LDAvis", "ggplot2", 'rlang', 'ranger'))
# Each comment is a .ipynb cell
# Install and load necessary packages
install.packages(c("tm", "SnowballC", "slam", "topicmodels", "quanteda", "caret",
"e1071", "randomForest", "kernlab", "cluster", "topicmodels",
"LDAvis", "ggplot2", 'rlang', 'ranger'))
library(tm)
library(slam)
library(quanteda)
library(caret)
library(e1071)
library(randomForest)
library(kernlab)
library(cluster)
library(topicmodels)
library(LDAvis)
library(ggplot2)
library(stringr)
library(tokenizers)
library(SnowballC)
library(ranger)
#
labelcount <- table(fulldata$label)
repeated <- names(labelcount[labelcount > 1])
# Load data
fulldata <- read.csv("fulldata-updated.csv")
fulldata <- fulldata[!duplicated(fulldata$title), ]
fulldata <- fulldata[order(rownames(fulldata)), ]
fulldata <- as.data.frame(lapply(fulldata, type.convert))
rownames(fulldata) <- NULL
#
labelcount <- table(fulldata$label)
repeated <- names(labelcount[labelcount > 1])
fulldata <- fulldata[fulldata$label %in% repeated, ]
fulldata <- droplevels(fulldata)  # Drop unused levels if needed
rownames(fulldata) <- seq_len(nrow(fulldata))
#
fulldata$date <- as.POSIXct(fulldata$date, format = "%Y-%m-%d %H:%M:%S")
fulldata$label <- as.factor(fulldata$label)
month_stats <- summary(as.numeric(format(fulldata$date, "%m")))
fulldata$article <- as.character(fulldata$article)
# Clean text
articles <- str_replace_all(fulldata$article, "\n", " ")
articles <- str_replace_all(articles, "[0-9]+", "")
articles <- str_replace_all(articles, "[,\\!?/:;''()``’“-”—#]", "")
articles <- str_replace_all(articles, "[.]+", "")
articles <- tolower(articles)
articles <- str_replace_all(articles, "\\b\\w\\b", "")
articles <- as.character(articles)
# Tokenizer
articles <- sapply(articles, function(x) tokenizers::tokenize_words(x))
articles <- as.list(articles)
stemmed_articles <- list()
for (i in 1:length(articles)) {
words <- c()
for (word in articles[[i]]) {
stemmed_word <- SnowballC::wordStem(word, "english")
words <- c(words, stemmed_word)
}
stemmed_articles[[i]] <- words
}
stemmed_articles <- as.list(stemmed_articles)
#
stopwordscustom <- read.csv('stp.csv', header = FALSE, col.names = c('word'))
stopwordscustom <- as.character(stopwordscustom$word)
# Remove custom stopwords
articles <- lapply(articles, function(x) x[!x %in% stopwordscustom])
# Generate a document-term matrix
dtm <- DocumentTermMatrix(Corpus(VectorSource(articles)))
# Transform to a term frequency-inverse document frequency (TF-IDF) matrix
dtm_tfidf <- weightTfIdf(dtm)
# Label encoding
fulldata$labelnumber <- as.numeric(as.factor(fulldata$label))
# Merge the labels with the dtm
dtm_tfidf <- cbind(dtm_tfidf, fulldata$labelnumber)
# Split the dataset into training and testing sets
set.seed(4545) #seed for the reproducibility
trainIndex <- createDataPartition(fulldata$labelnumber, p = .6, list = FALSE, times = 1)
# Split the labels as well
train_labels <- fulldata$labelnumber[trainIndex]
test_labels  <- fulldata$labelnumber[-trainIndex]
# Split the dtm_tfidf
train <- dtm_tfidf[trainIndex,]
test  <- dtm_tfidf[-trainIndex,]
library(Matrix)
# Train the model
model <- ranger(train_labels ~ ., data = data.frame(train),
importance = 'impurity', num.trees = 500)
# Train the model
model <- ranger(train_labels ~ ., data = as.data.frame(as.matrix(train)),
importance = 'impurity', num.trees = 500)
train_labels
as.data.frame(as.matrix(train))
# Train the model
model <- ranger(train_labels ~ ., data = as.data.frame(as.matrix(train)),
importance = 'impurity', num.trees = 500)
# Prepare matrices suitable for Random Forest
train_df <- as.data.frame(as.matrix(train))
test_df <- as.data.frame(as.matrix(test))
# Rename the columns to use only alphanumeric characters and underscores
names(train_df) <- make.names(names(train_df), unique = TRUE)
names(test_df) <- make.names(names(test_df), unique = TRUE)
# Train the model
model <- ranger(train_labels ~ ., data = train_df,
importance = 'impurity', num.trees = 500)
# Train the model
model <- ranger(train_labels ~ ., data = train_df,
importance = 'impurity', num.trees = 500)
# Predict on the test set
predictions <- predict(model, test_df)
# Evaluate model performance
table(predictions$predictions, test_labels)
# Print classification report
print(confusionMatrix(predictions, test$labelnumber))
# Print classification report
print(confusionMatrix(predictions, test_df$labelnumber))
# Print classification report
print(confusionMatrix(predictions, test_labels))
# Print classification report
print(confusionMatrix(predictions$predictions, test_labels))
# Evaluate model performance
table(predictions$predictions, test_labels)
predictions
### END OF ATTEMPT
accuracy <- sum(predictions == test_labels) / length(test_labels)
test_labels
predictions
### END OF ATTEMPT
accuracy <- sum(predictions$predictions == test_labels) / length(test_labels)
accuracy
# Evaluate model performance
table(predictions$predictions, test_labels)
test_labels
accuracy
### END OF ATTEMPT
accuracy <- sum(predictions$predictions == test_labels) / length(test_labels)
# Evaluate model performance
table(predictions$predictions, test_labels)
predictions
predictions$predictions
able(predictions$predictions, test_labels)
table(predictions$predictions, test_labels)
### END OF ATTEMPT
accuracy <- sum(round(predictions$predictions),0 == test_labels) / length(test_labels)
### END OF ATTEMPT
accuracy <- sum(round(predictions$predictions), 0 ) == test_labels) / length(test_labels)
### END OF ATTEMPT
accuracy <- sum(round(predictions$predictions, 0)  == test_labels) / length(test_labels)
accuracy
# Print classification report
confusionMatrix(predictions$predictions, test_labels)
predictions$predictions
test_labels
accuracy <- sum(round(predictions$predictions, 1)  == test_labels) / length(test_labels)
# Check accuracy
accuracy <- sum(round(predictions$predictions, 0)  == test_labels) / length(test_labels)
# Check accuracy
accuracy <- sum(round(predictions$predictions, 0)  == test_labels) / length(test_labels)
test_labels
predictions$predictions
test_labels
# Print classification report
confusionMatrix(predictions$predictions, test_labels)
predictions$predictions
# Print classification report
confusionMatrix(round(predictions$predictions, 0), test_labels)
round(predictions$predictions, 0)
test_labels
# Print classification report
confusionMatrix(as.factor(round(predictions$predictions, 0)), as.factor(test_labels))
as.factor(round(predictions$predictions, 0))
as.factor(test_labels)
# Run a Bagging model
control <- trainControl(method="bag", number=150)
model_bag <- train(as.factor(labelnumber) ~ ., data = train, trControl=control, method="treebag")
model_bag <- train(as.factor(labelnumber) ~ ., data = train_df, trControl=control, method="treebag")
View(fulldata)
model_bag <- train(as.factor(train_labels) ~ ., data = train_df, trControl=control, method="treebag")
# Run a Bagging model
control <- trainControl(method = "cv", number = 10) # Changed method to 'cv' for cross-validation and number to 10 for 10-fold cross-validation.
model_bag <- train(as.factor(train_labels) ~ ., data = train_df, trControl = control, method = "treebag")
