data$text <- str_replace_all(data$text, "[,\\!?/:;''()``’“-”—#]", "")
data$text <- str_replace_all(data$text, "[.]+", "")
data$text <- tolower(data$text)
data$text <- str_replace_all(data$text, "\\b\\w\\b", "")
data$text <- as.character(data$text)
return(data$text)
}
articles <- cleanText(myData)
################################# remove stopwords
remove_stopwords <- function(texts, stopwords) {
cleaned_texts <- lapply(texts, function(text) {
# Tokenize the text
tokens <- strsplit(tolower(text), "\\s+")
# Remove stopwords
tokens_clean <- tokens[[1]][!(tokens[[1]] %in% stopwords)]
# Join the cleaned tokens back into a text
cleaned_text <- paste(tokens_clean, collapse = " ")
# Return the cleaned text
return(cleaned_text)
})
return(cleaned_texts)
}
# Create custom stopwords
stopwordscustom <- read.csv('stp.csv', header = FALSE, col.names = c('word'))
stopwordscustom <- c(stopwordscustom$word, 'article', 'with')  # Add 'article' and 'with' to the custom stopwords list
articles_clean <- remove_stopwords(articles, stopwordscustom)
################################# stemming
# Stemming function
stem_articles <- function(articles) {
stemmed_articles <- character(length(articles))
for (i in 1:length(articles)) {
words <- c()
for (word in articles[[i]]) {
stemmed_word <- SnowballC::wordStem(word, "porter")
words <- c(words, stemmed_word)
}
stemmed_articles[i] <- paste(words, collapse = " ")
}
return(stemmed_articles)
}
stemmed_articles <- stem_articles(articles_clean)
# Create a Corpus
corpus <- Corpus(VectorSource(stemmed_articles))
# Create a Document-Term Matrix (DTM)
dtm <- DocumentTermMatrix(corpus, control = list(ngrams = c(1, 3)))
# Apply TF-IDF weighing
dtm_tfidf <- weightTfIdf(dtm)
# Label encoding
myData$labelnumber <- as.numeric(as.factor(myData$label))
# Merge the labels with the dtm_tfidf
dtm_tfidf <- cbind(dtm_tfidf, myData$labelnumber)
train_test_split <- function(dtm_tfidf, myData, partition_ratio) {
# Split the dataset into training and testing sets
trainIndex <- createDataPartition(myData$labelnumber, p = partition_ratio, list = FALSE, times = 1)
# Split the labels as well
train_labels <- myData$labelnumber[trainIndex]
test_labels  <- myData$labelnumber[-trainIndex]
# Split the dtm_tfidf
train <- dtm_tfidf[trainIndex,]
test <- dtm_tfidf[-trainIndex,]
# Prepare matrices suitable for Random Forest
train_df <- as.data.frame(as.matrix(train))
test_df <- as.data.frame(as.matrix(test))
colnames(train_df)[ncol(train_df)] <- "labelnumer"
colnames(test_df)[ncol(test_df)] <- "labelnumer"
# Rename the columns to use only alphanumeric characters and underscores
names(train_df) <- make.names(names(train_df), unique = TRUE)
names(test_df) <- make.names(names(test_df), unique = TRUE)
# Return the training and testing data frames and labels
return(list(train_df = train_df, test_df = test_df, train_labels = train_labels, test_labels = test_labels))
}
split_data <- train_test_split(dtm_tfidf, myData, 0.6)
train_df <- split_data$train_df
test_df <- split_data$test_df
train_labels <- split_data$train_labels
test_labels <- split_data$test_labels
# Train the model
model <- ranger(as.factor(train_labels) ~ ., data = train_df,
importance = 'impurity', num.trees = 500)
# Predict on the test set
predictions <- predict(model, test_df)
# Evaluate model performance
table(predictions$predictions, test_labels)
# Check accuracy
accuracy <- sum(predictions$predictions == test_labels) / length(test_labels)
# FIX ATTEMPT
# Define the levels that should exist
all_levels <- 1:11 # Adjust this to the levels we expect to have
# Convert predictions and test_labels to factor and explicitly set the levels
predictions_factor <- factor(predictions$predictions, levels=all_levels)
test_labels_factor <- factor(test_labels, levels=all_levels)
# Compute the confusion matrix
cm <- confusionMatrix(predictions_factor, test_labels_factor)
# Print the confusion matrix
print(cm)
accuracy
# Run a Bagging model
control <- trainControl(method = "cv", number = 2) # Changed method to 'cv' for cross-validation and number to 2 for 2-fold cross-validation, as it is computationally heavy.
model_bag <- caret::train(as.factor(labelnumer) ~ ., data=train_df, trControl=control, method="treebag")
predictions_bag <- predict(model_bag, newdata = test_df, type="raw")
# Compute the confusion matrix for bagging model
cm_bag <- confusionMatrix(predictions_bag, test_labels_factor)
# Print the confusion matrix
print(cm_bag)
# Run a LDA model and plot the topics
lda <- LDA(dtm, k = 20, control = list(seed = 3434), )
topics <- tidytext::tidy(lda, matrix = "beta")
top_terms <- topics %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
plot_lda <- top_terms %>%
mutate(term = tidytext::reorder_within(term, beta, topic)) %>%
ggplot(aes(beta, term, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
# coord_flip() +
theme_minimal() +
labs(title = "Top 10 terms in each LDA topic",
x = "Beta", y = "")
print(plot_lda)
plot_lda_topics <- function(articles, k, num_top_terms = 10, title = "Top Terms in Each LDA Topic", ngrams = c(1, 3)) {
# Create a Corpus
corpus <- Corpus(VectorSource(articles))
# Create a Document-Term Matrix (DTM)
dtm <- DocumentTermMatrix(corpus, control = list(ngrams = ngrams))
# Run LDA model
lda <- LDA(dtm, k = k, control = list(seed = 3434))
# Get the top terms for each topic
topics <- tidy(lda, matrix = "beta")
top_terms <- topics %>%
group_by(topic) %>%
top_n(num_top_terms, beta) %>%
ungroup() %>%
arrange(topic, -beta)
# Clean the terms in the top_terms tibble by removing quotes, commas, c(, and )
top_terms <- top_terms %>%
mutate(term = gsub("\"|,|\\bc\\(|\\)", "", term)) # Remove quotes, commas, c(, and )
# Plot the LDA topics
plot_lda <- top_terms %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(beta, term, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
theme_minimal() +
labs(title = title, x = "Beta", y = "")
print(plot_lda)
}
plot_lda_topics(articles = articles_clean, k = 5, num_top_terms = 8,
title = "Top 8 Terms in Each LDA Topic", ngrams = c(1, 3))
plot_lda_topics(articles = articles_clean, k = 10, num_top_terms = 10,
title = "Top 10 Terms in Each LDA Topic", ngrams = c(1, 3))
plot_lda_topics(articles = articles_clean, k = 10, num_top_terms = 10,
title = "Top 10 Terms in Each LDA Topic", ngrams = c(2, 3))
plot_lda_topics(articles = articles_clean, k = 10, num_top_terms = 10,
title = "Top 10 Terms in Each LDA Topic", ngrams = 3)
# ------------------
plot_lda_topics <- function(articles, k, num_top_terms = 10, title = "Top Terms in Each LDA Topic", ngrams = c(1, 3)) {
# Create a Corpus
corpus <- Corpus(VectorSource(articles))
# Create a Document-Term Matrix (DTM)
dtm <- DocumentTermMatrix(corpus, control = list(ngrams = ngrams))
# Extract n-grams from the DTM
ngram_terms <- findFreqTerms(dtm, lowfreq = 1)
# Run LDA model
lda <- LDA(dtm, k = k, control = list(seed = 3434))
# Get the top terms for each topic
topics <- tidy(lda, matrix = "beta")
top_terms <- topics %>%
filter(term %in% ngram_terms) %>%
group_by(topic) %>%
top_n(num_top_terms, beta) %>%
ungroup() %>%
arrange(topic, -beta)
# Clean the terms in the top_terms tibble by removing quotes, commas, c(, and )
top_terms <- top_terms %>%
mutate(term = gsub("\"|,|\\bc\\(|\\)", "", term)) # Remove quotes, commas, c(, and )
# Plot the LDA topics
plot_lda <- top_terms %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(beta, term, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
theme_minimal() +
labs(title = title, x = "Beta", y = "")
print(plot_lda)
}
plot_lda_topics(articles = articles_clean, k = 10, num_top_terms = 10,
title = "Top 10 Terms in Each LDA Topic", ngrams = 3)
# -----------------------
plot_lda_topics <- function(articles, k, num_top_terms = 10, title = "Top Terms in Each LDA Topic", ngrams = c(1, 3)) {
# Tokenize the articles into n-grams
ngram_tokenizer <- function(x) {
tokenizers::tokenize_ngrams(x, n = ngrams[1]:ngrams[2], delim = " ")
}
tokens <- lapply(articles, ngram_tokenizer)
# Create a Corpus
corpus <- Corpus(VectorSource(tokens))
# Create a Document-Term Matrix (DTM)
dtm <- DocumentTermMatrix(corpus)
# Run LDA model
lda <- LDA(dtm, k = k, control = list(seed = 3434))
# Get the top terms for each topic
topics <- tidy(lda, matrix = "beta")
top_terms <- topics %>%
group_by(topic) %>%
top_n(num_top_terms, beta) %>%
ungroup() %>%
arrange(topic, -beta)
# Clean the terms in the top_terms tibble by removing quotes, commas, c(, and )
top_terms <- top_terms %>%
mutate(term = gsub("\"|,|\\bc\\(|\\)", "", term)) # Remove quotes, commas, c(, and )
# Plot the LDA topics
plot_lda <- top_terms %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(beta, term, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
theme_minimal() +
labs(title = title, x = "Beta", y = "")
print(plot_lda)
}
plot_lda_topics(articles = articles_clean, k = 10, num_top_terms = 10,
title = "Top 10 Terms in Each LDA Topic", ngrams = 3)
inspect(dtm)
inspect(dtm)
inspect(dtm[202:205, 1:5])
inspect(dtm[202:205, 1:10])
inspect(dtm[202:205, 11:30])
inspect(dtm[202:205, 20:30])
inspect(dtm[202:205, 1100:1110])
inspect(dtm[202:205, 1500:1510])
# --------
BigramTokenizer12 <- function(x)unlist(lapply(ngrams(words(x), 1:2), paste, collapse = " "), use.names = FALSE)
corpus <- corpus(stemmed_articles)
tokens <- tokens(corpus, what = "word", ngrams = 1:3)
# Step 1: Tokenize the text with n-grams
tokens <- tokens(stemmed_articles, what = "word", ngrams = 1:3)
# --------
BigramTokenizer12 <- function(x)unlist(lapply(ngrams(words(x), 1:2), paste, collapse = " "), use.names = FALSE)
# Create a Corpus
corpus <- Corpus(VectorSource(stemmed_articles))
dtm.bigram <- DocumentTermMatrix(corpus, control=list(tokenize = BigramTokenizer12, wordLengths=c(1,Inf))) # control=list(wordLengths=c(1,Inf))
library(RTextTools)
install.packages('RTextTools')
library(RTextTools)
matrix <- create_matrix(stemmed_articles,ngramLength=3)
matrix
# --------
library("RWeka")
# --------
install.packages('RWeka')
library(RWeka)
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
tdm <- TermDocumentMatrix(stemmed_articles, control = list(tokenize = BigramTokenizer))
BigramTokenizer <-
function(x)
BigramTokenizer <-
function(x)
unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE)
tdm <- TermDocumentMatrix(stemmed_articles, control = list(tokenize = BigramTokenizer))
# Create a Document-Term Matrix (DTM)
dtm <- DocumentTermMatrix(corpus, control = list(ngrams = c(1, 3)))
dtm
corpus <- VCorpus(VectorSource(stemmed_articles))
tdm <- TermDocumentMatrix(corpus, control = list(tokenize = BigramTokenizer))
corpus <- Corpus(VectorSource(stemmed_articles))
tdm <- TermDocumentMatrix(corpus, control = list(tokenize = BigramTokenizer))
corpus <- VCorpus(VectorSource(stemmed_articles))
tdm <- TermDocumentMatrix(corpus, control = list(tokenize = BigramTokenizer))
# Create a Corpus
corpus <- VCorpus(VectorSource(stemmed_articles))
# Create a Document-Term Matrix (DTM)
dtm <- DocumentTermMatrix(corpus, control = list(ngrams = c(1, 3)))
dtm
# Create a Corpus
corpus <- VCorpus(VectorSource(stemmed_articles))
# Define a custom tokenizer function for bigrams and trigrams
myTokenizer <- function(x) {
unlist(tokenize_ngrams(tolower(x), n = 1:3))
}
# Create a Document-Term Matrix (DTM) with bigrams and trigrams
dtm <- DocumentTermMatrix(corpus, control = list(tokenize = myTokenizer))
# Create a Corpus
corpus <- VCorpus(VectorSource(stemmed_articles))
# Define a custom tokenizer function for bigrams and trigrams
myTokenizer <- function(x) {
NGramTokenizer(x, Weka_control(min = 1, max = 3))
}
# Create a Document-Term Matrix (DTM) with bigrams and trigrams
dtm <- DocumentTermMatrix(corpus, control = list(tokenize = myTokenizer))
dtm
inspect(dtm)
# -----------------------
plot_lda_topics <- function(articles, k, num_top_terms = 10, title = "Top Terms in Each LDA Topic", ngrams = c(1, 3)) {
# Create a Corpus
corpus <- VCorpus(VectorSource(stemmed_articles))
# Define a custom tokenizer function for bigrams and trigrams
myTokenizer <- function(x) {
NGramTokenizer(x, Weka_control(min = 1, max = 2))
}
# Create a Document-Term Matrix (DTM) with bigrams and trigrams
dtm <- DocumentTermMatrix(corpus, control = list(tokenize = myTokenizer))
# Run LDA model
lda <- LDA(dtm, k = k, control = list(seed = 3434))
# Get the top terms for each topic
topics <- tidy(lda, matrix = "beta")
top_terms <- topics %>%
group_by(topic) %>%
top_n(num_top_terms, beta) %>%
ungroup() %>%
arrange(topic, -beta)
# Clean the terms in the top_terms tibble by removing quotes, commas, c(, and )
top_terms <- top_terms %>%
mutate(term = gsub("\"|,|\\bc\\(|\\)", "", term)) # Remove quotes, commas, c(, and )
# Plot the LDA topics
plot_lda <- top_terms %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(beta, term, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
theme_minimal() +
labs(title = title, x = "Beta", y = "")
print(plot_lda)
}
plot_lda_topics(articles = articles_clean, k = 10, num_top_terms = 10,
title = "Top 10 Terms in Each LDA Topic", ngrams = 3)
# -----------------------
plot_lda_topics <- function(articles, k, num_top_terms = 10, title = "Top Terms in Each LDA Topic", min_ngram=1, max_ngram=2) {
# Create a Corpus
corpus <- VCorpus(VectorSource(stemmed_articles))
# Define a custom tokenizer function for bigrams and trigrams
myTokenizer <- function(x) {
NGramTokenizer(x, Weka_control(min = min_ngram, max = max_ngram))
}
# Create a Document-Term Matrix (DTM) with bigrams and trigrams
dtm <- DocumentTermMatrix(corpus, control = list(tokenize = myTokenizer))
# Run LDA model
lda <- LDA(dtm, k = k, control = list(seed = 3434))
# Get the top terms for each topic
topics <- tidy(lda, matrix = "beta")
top_terms <- topics %>%
group_by(topic) %>%
top_n(num_top_terms, beta) %>%
ungroup() %>%
arrange(topic, -beta)
# Clean the terms in the top_terms tibble by removing quotes, commas, c(, and )
#top_terms <- top_terms %>%
# mutate(term = gsub("\"|,|\\bc\\(|\\)", "", term)) # Remove quotes, commas, c(, and )
# Plot the LDA topics
plot_lda <- top_terms %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(beta, term, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
theme_minimal() +
labs(title = title, x = "Beta", y = "")
print(plot_lda)
}
plot_lda_topics(articles = articles_clean, k = 10, num_top_terms = 10,
title = "Top 10 Terms in Each LDA Topic", min_ngram = 1, max_ngram = 1)
plot_lda_topics(articles = articles_clean, k = 10, num_top_terms = 10,
title = "Top 10 Terms in Each LDA Topic", min_ngram = 1, max_ngram = 2)
plot_lda_topics(articles = articles_clean, k = 10, num_top_terms = 10,
title = "Top 10 Terms in Each LDA Topic", min_ngram = 1, max_ngram = 3)
plot_lda_topics(articles = articles_clean, k = 10, num_top_terms = 10,
title = "Top 10 Terms in Each LDA Topic", min_ngram = 2, max_ngram = 2)
plot_lda_topics(articles = articles_clean, k = 10, num_top_terms = 10,
title = "Top 10 Terms in Each LDA Topic", min_ngram = 1, max_ngram = 2)
stemmed_articles <- stem_articles(articles_clean)
# Create a Corpus
corpus <- VCorpus(VectorSource(stemmed_articles))
# Define a custom tokenizer function for bigrams and trigrams
myTokenizer <- function(x) {
NGramTokenizer(x, Weka_control(min = 1, max = 2))
}
# Create a Document-Term Matrix (DTM) with bigrams and trigrams
dtm <- DocumentTermMatrix(corpus, control = list(tokenize = myTokenizer))
# Apply TF-IDF weighing
dtm_tfidf <- weightTfIdf(dtm)
# Apply TF-IDF weighing
dtm_tfidf <- weightTfIdf(dtm)
# Create a Corpus
corpus <- Corpus(VectorSource(stemmed_articles))
# Create a Corpus
corpus <- VCorpus(VectorSource(stemmed_articles))
# Define a custom tokenizer function for bigrams and trigrams
myTokenizer <- function(x) {
NGramTokenizer(x, Weka_control(min = 1, max = 2))
}
# Create a Document-Term Matrix (DTM) with bigrams and trigrams
dtm <- DocumentTermMatrix(corpus, control = list(tokenize = myTokenizer))
# Apply TF-IDF weighing
dtm_tfidf <- weightTfIdf(dtm)
# Label encoding
myData$labelnumber <- as.numeric(as.factor(myData$label))
# Merge the labels with the dtm_tfidf
dtm_tfidf <- cbind(dtm_tfidf, myData$labelnumber)
train_test_split <- function(dtm_tfidf, myData, partition_ratio) {
# Split the dataset into training and testing sets
trainIndex <- createDataPartition(myData$labelnumber, p = partition_ratio, list = FALSE, times = 1)
# Split the labels as well
train_labels <- myData$labelnumber[trainIndex]
test_labels  <- myData$labelnumber[-trainIndex]
# Split the dtm_tfidf
train <- dtm_tfidf[trainIndex,]
test <- dtm_tfidf[-trainIndex,]
# Prepare matrices suitable for Random Forest
train_df <- as.data.frame(as.matrix(train))
test_df <- as.data.frame(as.matrix(test))
colnames(train_df)[ncol(train_df)] <- "labelnumer"
colnames(test_df)[ncol(test_df)] <- "labelnumer"
# Rename the columns to use only alphanumeric characters and underscores
names(train_df) <- make.names(names(train_df), unique = TRUE)
names(test_df) <- make.names(names(test_df), unique = TRUE)
# Return the training and testing data frames and labels
return(list(train_df = train_df, test_df = test_df, train_labels = train_labels, test_labels = test_labels))
}
split_data <- train_test_split(dtm_tfidf, myData, 0.6)
train_df <- split_data$train_df
test_df <- split_data$test_df
train_labels <- split_data$train_labels
test_labels <- split_data$test_labels
# Train the model
model <- ranger(as.factor(train_labels) ~ ., data = train_df,
importance = 'impurity', num.trees = 500)
# Predict on the test set
predictions <- predict(model, test_df)
# Evaluate model performance
table(predictions$predictions, test_labels)
# Check accuracy
accuracy <- sum(predictions$predictions == test_labels) / length(test_labels)
accuracy
# Define the levels that should exist
all_levels <- 1:11 # Adjust this to the levels we expect to have
# Convert predictions and test_labels to factor and explicitly set the levels
predictions_factor <- factor(predictions$predictions, levels=all_levels)
test_labels_factor <- factor(test_labels, levels=all_levels)
# Compute the confusion matrix
cm <- confusionMatrix(predictions_factor, test_labels_factor)
# Print the confusion matrix
print(cm)
stemmed_articles <- stem_articles(articles_clean)
# Create a Corpus
corpus <- VCorpus(VectorSource(stemmed_articles))
# Define a custom tokenizer function
myTokenizer <- function(x) {
NGramTokenizer(x, Weka_control(min = 1, max = 1))
}
# Create a Document-Term Matrix (DTM) with bigrams and trigrams
dtm <- DocumentTermMatrix(corpus, control = list(tokenize = myTokenizer))
# Apply TF-IDF weighing
dtm_tfidf <- weightTfIdf(dtm)
# Label encoding
myData$labelnumber <- as.numeric(as.factor(myData$label))
# Merge the labels with the dtm_tfidf
dtm_tfidf <- cbind(dtm_tfidf, myData$labelnumber)
train_test_split <- function(dtm_tfidf, myData, partition_ratio) {
# Split the dataset into training and testing sets
trainIndex <- createDataPartition(myData$labelnumber, p = partition_ratio, list = FALSE, times = 1)
# Split the labels as well
train_labels <- myData$labelnumber[trainIndex]
test_labels  <- myData$labelnumber[-trainIndex]
# Split the dtm_tfidf
train <- dtm_tfidf[trainIndex,]
test <- dtm_tfidf[-trainIndex,]
# Prepare matrices suitable for Random Forest
train_df <- as.data.frame(as.matrix(train))
test_df <- as.data.frame(as.matrix(test))
colnames(train_df)[ncol(train_df)] <- "labelnumer"
colnames(test_df)[ncol(test_df)] <- "labelnumer"
# Rename the columns to use only alphanumeric characters and underscores
names(train_df) <- make.names(names(train_df), unique = TRUE)
names(test_df) <- make.names(names(test_df), unique = TRUE)
# Return the training and testing data frames and labels
return(list(train_df = train_df, test_df = test_df, train_labels = train_labels, test_labels = test_labels))
}
split_data <- train_test_split(dtm_tfidf, myData, 0.6)
train_df <- split_data$train_df
test_df <- split_data$test_df
train_labels <- split_data$train_labels
test_labels <- split_data$test_labels
# Train the model
model <- ranger(as.factor(train_labels) ~ ., data = train_df,
importance = 'impurity', num.trees = 500)
# Predict on the test set
predictions <- predict(model, test_df)
# Evaluate model performance
table(predictions$predictions, test_labels)
# Check accuracy
accuracy <- sum(predictions$predictions == test_labels) / length(test_labels)
# Define the levels that should exist
all_levels <- 1:11 # Adjust this to the levels we expect to have
accuracy
# Define the levels that should exist
all_levels <- 1:11 # Adjust this to the levels we expect to have
# Convert predictions and test_labels to factor and explicitly set the levels
predictions_factor <- factor(predictions$predictions, levels=all_levels)
test_labels_factor <- factor(test_labels, levels=all_levels)
# Compute the confusion matrix
cm <- confusionMatrix(predictions_factor, test_labels_factor)
# Print the confusion matrix
print(cm)
# Run a Bagging model
control <- trainControl(method = "cv", number = 2) # Changed method to 'cv' for cross-validation and number to 2 for 2-fold cross-validation, as it is computationally heavy.
model_bag <- caret::train(as.factor(labelnumer) ~ ., data=train_df, trControl=control, method="treebag")
predictions_bag <- predict(model_bag, newdata = test_df, type="raw")
# Compute the confusion matrix for bagging model
cm_bag <- confusionMatrix(predictions_bag, test_labels_factor)
# Print the confusion matrix
print(cm_bag)
