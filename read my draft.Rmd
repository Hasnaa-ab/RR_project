---
title: "TextCorpus"
author: "Hassnaa Abdelghany"
date: "2023-06-14"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r include=FALSE}
# Function to install package if not already installed
installPackage <- function(packageName) {
  if (!(packageName %in% installed.packages())) {
    install.packages(packageName)
  }
}

# Install and load necessary packages
requiredPackages <- c(
  "tm", "SnowballC", "slam", "topicmodels", "quanteda", 
  "caret", "e1071", "randomForest", "kernlab", "cluster", 
  "topicmodels", "LDAvis", "ggplot2", "rlang", "ranger", "Matrix",
  "RWeka", "caret", "stringr", "tokenizers"
)

# Install required packages if not already installed
invisible(sapply(requiredPackages, installPackage))

# Load the installed packages
lapply(requiredPackages, require, character.only = TRUE)

```
this function should read all text files in a folder and merge them in one data frame
```{r}

################## function to read txt and return data frame
multiTextFile <- function(directoryPath) {
  # Get the list of file names in the directory
  fileNames <- list.files(path = directoryPath, pattern = "\\.txt$", full.names = TRUE)
  
  # Create an empty list to store the data frames
  dataFrames <- list()
  
  # Read each file and store it as a separate data frame
  for (filePath in fileNames) {
    data <- read.table(filePath, header = TRUE)  # Modify read function based on your file format
    
    # Add the data frame to the list
    dataFrames <- c(dataFrames, list(data))
  }
  
  # Merge the data frames into a single data frame
  mergedData <- do.call(rbind, dataFrames)
  names(mergedData) <- 'text'
  mergedData <- mergedData[!duplicated(mergedData$text), ]
  
  
  # Return the merged data frame
  return(mergedData)
}
```
if the data is in csv / table format we can read the file via csvText(), were file is the file name, textCol is the column name with text and, lableCol is the column with text lables or category that we would like to train the model to learn to classify future text according to them.
all parameters should be in string format.

```{r}

############################# function to read csv with text

csvText <- function(file, textCol, labelCol) {
  
  data <- read.csv(file)
  data = data[,c(textCol,labelCol)]
  names(data) = c('text','label')
  names(data[,labelCol]) = 'label'
  data <- data[!duplicated(data$text), ]
  data$text = as.character(data$text)
  data$label = as.factor(data$label)
  
  return(data)
  
}
```
the function well return data frame and rename the text column as text and label column as label. make sure your data does not contain other columns with the same names.
```{r}
myData = csvText('G:\\My Drive\\DSBA\\RR\\localrepos\\RR_project\\fulldata-updated.csv', 'title', 'label')
head(myData)
```
if you have only data frame with text column, you can combine label column from anther vector using assignLabels where df is orginal text dataframe and labels is label vector you would like to merge.
```{r}
######################### merge label col to data frame

assignLabels <- function(df,labels) {
  
  df <- cbind(df,labels)
  names(df) = c('text', 'label')
  df <- df[!duplicated(df$text), ]
  df$text = as.character(df$text)
  df$label = as.factor(df$label)
  return(df)
}
```
to avoid error when creating train and test sample use validLabels(). it takes one argument which is data and make sure there is at least two labels for each label in the data(one to be assigned in test set and another in train set). this avoid model to face labels for the first time when training or predicting data.
```{r}
######################drop unique labels

validLabels <- function(df) {
  labelcount <- table(df$label)
  repeated <- names(labelcount[labelcount > 1])
  df <- df[df$label %in% repeated, ]
  df <- droplevels(df)  # Drop unused levels if needed
  rownames(df) <- seq_len(nrow(df))
  return(df)
}
myData = validLabels(myData)
```
Next we shall clean text. The cleanText function is designed to perform text cleaning operations on a data frame containing text data. It takes the input data frame and modifies the text column by applying a series of cleaning steps. the function replaces all newline characters (\n) in the text column with a space, removes any numeric digits present in the text column and, remove characters such as commas, backslashes, exclamation marks, question marks, colons, semicolons, quotation marks, parentheses, hyphens, and hashtags. 
```{r}
############################ cleaning Text

cleanText <- function(data) {
  
  data$text <- str_replace_all(data$text, "\n", " ")
  data$text <- str_replace_all(data$text, "[0-9]+", "")
  data$text <- str_replace_all(data$text, "[,\\!?/:;''()``’“-”—#]", "")
  data$text <- str_replace_all(data$text, "[.]+", "")
  data$text <- tolower(data$text)
  data$text <- str_replace_all(data$text, "\\b\\w\\b", "")
  data$text <- as.character(data$text)
  
  return(data)
}

myData = cleanText(myData)
head(myData)
```
The createTokens function is designed to preprocess textual data in the form of a data frame. It takes the data data frame as input, which contains a column named text holding the textual content. Additionally, an optional argument stopwordscustom_file can be provided to specify a file containing custom stopwords. 
Custom Stopword Removal: The custom stopwords are read from the specified CSV file using read.csv and converted into a character vector. The removeWords function from the tm package is then used to remove these custom stopwords from the text in the text column.
N-gram Tokenization: The tokenize_ngrams function is employed to generate n-grams from the preprocessed text in the text column. In this case, bi-grams (n = 2) are created by setting n = 2 and n_min = 2. N-grams capture sequences of n words and help to capture contextual relationships between words.
```{r}
################################# create tokens


createTokens <- function(data, stopwordscustom_file = NULL) {

  stopwordscustom <- as.character(read.csv(stopwordscustom_file))
  data$text = lapply(data$text, function(x) tm::removeWords(x, stopwordscustom))
  # data$text <-lapply(data$text, function(x) stemDocument(x))
  data$text = tokenize_ngrams(data$text, n = 2, n_min = 2, simplify = T)
  data$text = as.list(data$text)
  return(data)
}

myData = createTokens(myData, 'G:\\My Drive\\DSBA\\RR\\localrepos\\RR_project\\stp.csv')
head(myData)
```
The function generateDTM takes a data frame data as input and performs the following steps:

It generates a document-term matrix (dtm) using the DocumentTermMatrix function from the tm package. This matrix represents the frequency of terms (words) in the text documents.

It transforms the document-term matrix dtm into a term frequency-inverse document frequency (TF-IDF) matrix (dtm_tfidf) using the weightTfIdf function. TF-IDF is a numerical statistic that reflects the importance of a term in a document relative to a collection of documents.

It merges the labels from the data data frame with the TF-IDF matrix dtm_tfidf using the cbind function. This combines the label information with the corresponding document representation.

Finally, the function returns the resulting TF-IDF matrix dtm_tfidf for analysis it will be converted to dataframe.
```{r warning=FALSE}
generateDTM <- function(data) {

  # Generate a document-term matrix
  dtm <- DocumentTermMatrix(VCorpus(VectorSource(data$text)),
                            control = list(tokenize = NGramTokenizer, ngrams = 2)) #controling for ngrams
  
  # Transform to a term frequency-inverse document frequency (TF-IDF) matrix
  dtm_tfidf <- weightTfIdf(dtm)
  dtm_tfidf$v <- as.numeric(dtm_tfidf$v)

  # Merge the labels with the dtm
  dtm_tfidf <- cbind(dtm_tfidf, data$label)
  #
  createDataPartition(data$label, p = 0.7) -> section
  
  dtm_tfidf %>% as.matrix() %>% as.data.frame() -> modeldata
  names(modeldata) <- unlist(dtm_tfidf$dimnames[2])
  names(modeldata)[modeldata %>% colnames() %>% length()] = 'label'
  type.convert(modeldata) -> modeldata
  modeldata$label %>% as.factor() -> modeldata$label
  modeldata[modeldata %>% is.na()] <- 0
  

  
  return(modeldata)
}

generateDTM(myData) -> modeldata

```
Now we are splitting data for training and testing sets.

The function first renames the columns of the data except for the first column (which is assumed to be the label column) using the make.names() function from the dplyr package. This is done to ensure that the column names are valid and unique.

Next, the function uses the createDataPartition() function from the caret package to create a random partition of the data based on the specified proportion prc. The list = FALSE argument ensures that a single set of indices is returned, and the times = 1 argument ensures that the partition is created only once.

The resulting partition indices are then used to subset the data into two sets: train and test. The train set contains the rows from data that correspond to the partition indices, while the test set contains the remaining rows.

The gc() function is called to perform garbage collection, freeing up memory resources that are no longer needed.

Finally, the function returns a list containing the train and test sets.
```{r}

  createSets <- function(data, prc) {
    names(data)[-1] <- names(data)[-1] %>%  make.names()
    trainIndex <- createDataPartition(data$label, p = prc, list = FALSE, times = 1)
    train <- data[trainIndex,]
    test  <- data[-trainIndex,]
    gc()
    return(list(train,test))
    
  }

  createSets(modeldata, 0.7) -> sets
  sets[[1]]  -> train_df
  sets[[2]]  -> test_df
```
The function takes four arguments: dfTrain and dfTest, which represent the training and testing datasets, respectively, nTrees which specifies the number of trees to be used in the random forest model, and packageUsed, which specifies the package to be used for training the model. By default, the function assumes the "ranger" package if no value is provided for packageUsed.

Inside the function, there is a conditional statement that checks the value of packageUsed. If it is set to "randomForest", the function trains a random forest model using the randomForest() function. The formula label ~ . specifies that the first column of the dataset (label) is the target variable, and all other columns are used as predictors. The resulting model is assigned to the variable model.

If packageUsed is set to "ranger", the function trains a model using the ranger() function. The as.formula(label ~ ., train_df) specifies the same formula as before. The importance = 'impurity' argument indicates that importance should be calculated based on impurity. The resulting model is assigned to the variable model.

Finally, the function returns the trained model.

After defining the trainModel function, the code snippet calls it with train_df and test_df as the training and testing datasets, respectively. The number of trees is set to 60. The resulting model is assigned to the variable theModel.

```{r}

  trainModel = function(dfTrain, dfTest, nTrees, packageUsed = 'ranger'){
  
    if (packageUsed == 'randomForest'){
        randomForest(label ~ .,
                     data = train_df,
                     type = 'classification',
                     ntree = nTrees) -> model
      
    }
    if (packageUsed == 'ranger'){
        ranger(as.formula(label ~ ., train_df),
               data = train_df,importance = 'impurity',
               num.trees = nTrees) -> model
      
    }
    return(model)
  }
  
  trainModel(train_df,test_df,nTrees = 60) -> theModel
  
```


#   sapply(modeldata[,-1], function(x) try(sum(x %>% as.numeric()))  ) -> totalTf_Idtf
# totalTf_Idtf %>% density() %>% plot(., xlim = c(0,30))
  
```


```{r}
### ATTEMPT TO FIX PROBLEMS

# Train the model
model <- ranger(label ~ ., data =train_df, 
                importance = 'impurity', num.trees = 20)

# Predict on the test set
predictions <- predict(model, test_df)

# Evaluate model performance
table(predictions$predictions, test_labels)

# Check accuracy
accuracy <- sum(round(predictions$predictions, 0)  == test_labels) / length(test_labels)


# Run a Bagging model
control <- trainControl(method = "cv", number = 2) # Changed method to 'cv' for cross-validation and number to 2 for 2-fold cross-validation, as it is computationally heavy.
model_bag <- train(as.factor(train_labels) ~ ., data = train_df, trControl = control, method = "treebag")
predictions_bag <- predict(model_bag, newdata = test_df)

# Print classification report
print(confusionMatrix(predictions_bag, test_df$labelnumber))

# Run a LDA model and plot the topics
lda <- LDA(train_df[, !colnames(train_df) %in% "labelnumber"], k = 20, control = list(seed = 3434))
topics <- tidy(lda, matrix = "beta")
top_terms <- topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)
plot_lda <- top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip() +
  theme_minimal() +
  labs(title = "Top 10 terms in each LDA topic",
       x = "Beta", y = "")
print(plot_lda)

```

